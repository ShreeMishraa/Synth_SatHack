{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej-RCpx_siKK"
   },
   "source": [
    "# Catastrophe bond Price Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1dptyZ1siKP"
   },
   "source": [
    "## 1. Data Collection and Preprocessing\n",
    "### 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19jBb471CTj5",
    "outputId": "10556b38-442a-4a4d-8b55-369d927505e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ta in /opt/anaconda3/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from ta) (1.24.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from ta) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->ta) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->ta) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries in Colab\n",
    "!pip install ta\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import yfinance as yf  # For downloading stock data\n",
    "import ta  # For technical analysis indicators\n",
    "from sklearn.preprocessing import RobustScaler  # For scaling features\n",
    "from sklearn.model_selection import TimeSeriesSplit  # For time series cross-validation\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # Gradient Boosting model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error  # For model evaluation\n",
    "from sklearn.feature_selection import SelectFromModel  # For feature selection\n",
    "from sklearn.pipeline import Pipeline  # For creating a model pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # For custom transformers\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values\n",
    "from sklearn.feature_selection import RFECV, mutual_info_regression\n",
    "from xgboost import XGBRegressor  # XGBoost model\n",
    "from lightgbm import LGBMRegressor  # LightGBM model\n",
    "from keras.models import Sequential  # For building neural networks\n",
    "from keras.layers import LSTM, Dense, Dropout  # Layers for LSTM model\n",
    "from keras.optimizers import Adam  # Adam optimizer for neural networks\n",
    "import optuna  # For hyperparameter optimization\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX  # For SARIMA modeling\n",
    "from prophet import Prophet  # Facebook's Prophet model for time series forecasting\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import shap  # For model interpretability\n",
    "from textblob import TextBlob  # For sentiment analysis\n",
    "import requests  # For making HTTP requests\n",
    "import json\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing  # For Holt-Winters' exponential smoothing\n",
    "from pmdarima import auto_arima  # For automatic ARIMA modeling\n",
    "from sklearn.linear_model import ElasticNet  # Elastic Net regression\n",
    "from sklearn.ensemble import VotingRegressor  # For ensemble modeling\n",
    "from datetime import datetime, timedelta  # For date and time operations\n",
    "import pytz  # For timezone handling\n",
    "import torch  # PyTorch for deep learning\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "from torch.utils.data import DataLoader, TensorDataset  # For data loading\n",
    "from transformers import TimeSeriesTransformerModel, TimeSeriesTransformerConfig  # For time series transformers\n",
    "import gym  # For reinforcement learning environments\n",
    "from stable_baselines3 import PPO  # Proximal Policy Optimization algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv  # For vectorized environments\n",
    "import warnings  # For handling warnings\n",
    "from sklearn.feature_selection import RFECV  # Recursive Feature Elimination with Cross-Validation\n",
    "from sklearn.linear_model import LassoCV  # Lasso regression with built-in cross-validation\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19jBb471CTj5",
    "outputId": "10556b38-442a-4a4d-8b55-369d927505e9"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_enhanced_targets(df):\n",
    "    \"\"\"\n",
    "    Create multiple target variables for enhanced prediction accuracy.\n",
    "    \"\"\"\n",
    "    # Create risk-adjusted spread\n",
    "    df['risk_adjusted_spread'] = df['spread_bps'] * (1 + df['insurance_risk'])\n",
    "    \n",
    "    # Calculate probability of attachment using expected loss and insurance risk\n",
    "    df['probability_of_attachment'] = df['expected_loss'] * (1 - np.exp(-df['insurance_risk']))\n",
    "    \n",
    "    # Calculate conditional expected loss\n",
    "    df['conditional_expected_loss'] = df['expected_loss'] / df['probability_of_attachment'].clip(lower=0.01)\n",
    "    \n",
    "    # Calculate market relative value\n",
    "    df['market_relative_value'] = df['spread_bps'] / df['expected_loss'].clip(lower=0.01)\n",
    "    \n",
    "    return {\n",
    "        'expected_loss': df['expected_loss'],\n",
    "        'risk_adjusted_spread': df['risk_adjusted_spread'],\n",
    "        'probability_of_attachment': df['probability_of_attachment'],\n",
    "        'conditional_expected_loss': df['conditional_expected_loss'],\n",
    "        'market_relative_value': df['market_relative_value']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS_mNPYLsiKT"
   },
   "source": [
    "### 1.2 Fetching Bitcoin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bykl1WP4C3Fv",
    "outputId": "ca0615b5-8fed-4295-917d-665007d1318e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from alphaized_data.csv\n",
      "Available columns in dataset: ['Date', 'collat_yield', 'insurance_risk', 'expected_loss', 'Temperature', 'Precipitation']\n",
      "\n",
      "Created target variables: ['expected_loss', 'log_expected_loss', 'risk_level', 'insurance_risk', 'risk_adjusted_loss']\n",
      "\n",
      "Using feature columns: ['collat_yield', 'Temperature', 'Precipitation']\n",
      "\n",
      "Data shapes:\n",
      "Training Features: (222, 3)\n",
      "Test Features: (56, 3)\n",
      "Training Target: (222,)\n",
      "Test Target: (56,)\n",
      "Loading data from alphaized_data.csv\n",
      "Available columns in dataset: ['Date', 'collat_yield', 'insurance_risk', 'expected_loss', 'Temperature', 'Precipitation']\n",
      "\n",
      "Created target variables: ['expected_loss', 'log_expected_loss', 'risk_level', 'insurance_risk', 'risk_adjusted_loss']\n",
      "\n",
      "Using feature columns: ['collat_yield', 'Temperature', 'Precipitation']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "def get_enhanced_targets(df):\n",
    "    \"\"\"\n",
    "    Create multiple target variables for enhanced prediction accuracy based on available columns.\n",
    "    First prints available columns and then creates appropriate target variables.\n",
    "    \"\"\"\n",
    "    print(\"Available columns in dataset:\", df.columns.tolist())\n",
    "    \n",
    "    targets = {}\n",
    "    \n",
    "    # Base target - expected loss\n",
    "    if 'expected_loss' in df.columns:\n",
    "        targets['expected_loss'] = df['expected_loss']\n",
    "        \n",
    "        # Log-transformed expected loss\n",
    "        targets['log_expected_loss'] = np.log1p(df['expected_loss'].clip(lower=0.0001))\n",
    "        \n",
    "        # Risk levels (1-5)\n",
    "        targets['risk_level'] = pd.qcut(df['expected_loss'], q=5, labels=[1,2,3,4,5]).astype(float)\n",
    "    \n",
    "    # Insurance risk based targets\n",
    "    if 'insurance_risk' in df.columns:\n",
    "        targets['insurance_risk'] = df['insurance_risk']\n",
    "        \n",
    "        if 'expected_loss' in df.columns:\n",
    "            targets['risk_adjusted_loss'] = df['expected_loss'] * (1 + df['insurance_risk'])\n",
    "    \n",
    "    # First loss probability based targets\n",
    "    if 'probability_first_loss' in df.columns:\n",
    "        targets['probability_first_loss'] = df['probability_first_loss']\n",
    "        \n",
    "        if 'expected_loss' in df.columns:\n",
    "            # Conditional loss given first loss\n",
    "            targets['conditional_loss'] = df['expected_loss'] / df['probability_first_loss'].clip(lower=0.0001)\n",
    "    \n",
    "    # ROLX based targets\n",
    "    if 'rolx' in df.columns:\n",
    "        targets['rolx'] = df['rolx']\n",
    "        \n",
    "        if 'expected_loss' in df.columns:\n",
    "            targets['risk_premium'] = df['rolx'] * df['expected_loss']\n",
    "    \n",
    "    # Create additional derived targets if needed\n",
    "    if len(targets) < 3:\n",
    "        print(\"Warning: Creating additional derived targets from available data.\")\n",
    "        \n",
    "        base_target = list(targets.values())[0]\n",
    "        base_name = list(targets.keys())[0]\n",
    "        \n",
    "        # Rolling statistics\n",
    "        rolling_window = min(10, len(df) // 5)\n",
    "        targets[f'{base_name}_ma'] = base_target.rolling(window=rolling_window, min_periods=1).mean()\n",
    "        targets[f'{base_name}_volatility'] = base_target.rolling(window=rolling_window, min_periods=1).std()\n",
    "        targets[f'{base_name}_roc'] = base_target.pct_change().fillna(0)\n",
    "    \n",
    "    if len(targets) < 2:\n",
    "        raise ValueError(\"Insufficient data for multi-target prediction. Need at least 2 targets.\")\n",
    "    \n",
    "    print(\"\\nCreated target variables:\", list(targets.keys()))\n",
    "    \n",
    "    # Clean up any infinite or null values\n",
    "    for key in targets:\n",
    "        targets[key] = targets[key].replace([np.inf, -np.inf], np.nan)\n",
    "        targets[key] = targets[key].fillna(targets[key].median())\n",
    "    \n",
    "    return targets\n",
    "\n",
    "def get_training_data(csv_file_path, target_name='expected_loss', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load and preprocess the training data for a specific target variable.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_file_path : str\n",
    "        Path to the CSV file containing the data\n",
    "    target_name : str\n",
    "        Name of the target variable to use (must be one of the enhanced targets)\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing X_train, X_test, y_train, y_test for the specified target\n",
    "    \"\"\"\n",
    "    # Load the CSV data\n",
    "    print(f\"Loading data from {csv_file_path}\")\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Generate enhanced targets\n",
    "    targets = get_enhanced_targets(df)\n",
    "    \n",
    "    if target_name not in targets:\n",
    "        raise ValueError(f\"Target '{target_name}' not found in available targets: {list(targets.keys())}\")\n",
    "    \n",
    "    # Drop the target columns and Date from features\n",
    "    all_target_columns = list(targets.keys())\n",
    "    feature_columns = [col for col in df.columns if col not in all_target_columns + ['Date']]\n",
    "    \n",
    "    if not feature_columns:\n",
    "        raise ValueError(\"No feature columns remaining after removing targets.\")\n",
    "    \n",
    "    print(\"\\nUsing feature columns:\", feature_columns)\n",
    "    X = df[feature_columns]\n",
    "    y = targets[target_name]\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_columns': feature_columns,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = 'alphaized_data.csv'\n",
    "    \n",
    "    # Get training data for expected loss prediction\n",
    "    train_data = get_training_data(csv_file_path, target_name='expected_loss')\n",
    "    \n",
    "    # Access the split data\n",
    "    X_train = train_data['X_train']\n",
    "    X_test = train_data['X_test']\n",
    "    y_train = train_data['y_train']\n",
    "    y_test = train_data['y_test']\n",
    "    \n",
    "    # Display shapes\n",
    "    print(\"\\nData shapes:\")\n",
    "    print(\"Training Features:\", X_train.shape)\n",
    "    print(\"Test Features:\", X_test.shape)\n",
    "    print(\"Training Target:\", y_train.shape)\n",
    "    print(\"Test Target:\", y_test.shape)\n",
    "    \n",
    "    # Get training data for a different target if needed\n",
    "    risk_data = get_training_data(csv_file_path, target_name='risk_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhTUUPRysiKV"
   },
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9fSb-_VbNKgx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Custom transformer for Recursive Feature Elimination with Cross-Validation\n",
    "class RFECVTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that performs feature selection using SelectFromModel.\n",
    "\n",
    "    This transformer is designed to work with scikit-learn's pipeline and\n",
    "    cross-validation tools.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, step=1, cv=5):\n",
    "\n",
    "        self.estimator = estimator\n",
    "        self.step = step\n",
    "        self.cv = cv\n",
    "        self.selector = SelectFromModel(estimator=estimator, threshold='median')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.selector.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "# Function to calculate double exponential moving average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLt-uAxzsiKX"
   },
   "source": [
    "### 2.2 Technical Indicators and Custom Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TNbFWFFxNKeP"
   },
   "outputs": [],
   "source": [
    "def engineer_features(self):\n",
    "    \"\"\"Enhanced feature engineering based on available columns\"\"\"\n",
    "    df = self.data.copy()\n",
    "\n",
    "    # Basic transformations and risk metrics\n",
    "    df['el_squared'] = df['expected_loss'] ** 2\n",
    "    df['insurance_risk_squared'] = df['insurance_risk'] ** 2\n",
    "\n",
    "    # Market cycle interactions\n",
    "    df['el_market'] = df['expected_loss'] * df['insurance_risk']\n",
    "\n",
    "    # Risk premium calculations\n",
    "    df['risk_premium'] = df['spread_bps'] - df['expected_loss_bps']\n",
    "    df['risk_premium_ratio'] = df['risk_premium'] / df['expected_loss_bps']\n",
    "\n",
    "    # Market condition indicators using collat_yield as a proxy for market stress\n",
    "    df['market_stress'] = df['insurance_risk'] * df['collat_yield']\n",
    "\n",
    "    # Climate risk interaction\n",
    "    df['climate_risk'] = (df['Temperature'] * df['Precipitation']) / \\\n",
    "                         (df['Temperature'].mean() * df['Precipitation'].mean())\n",
    "\n",
    "    # Time-based features\n",
    "    if 'Date' in df.columns:\n",
    "        df['month'] = df['Date'].dt.month\n",
    "        df['year'] = df['Date'].dt.year\n",
    "        df['quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "    # Probability of First Loss (PFL) already exists\n",
    "    if 'probability_first_loss' in df.columns:\n",
    "        df['pfl'] = df['probability_first_loss']\n",
    "\n",
    "    # ROLX already exists in the dataset\n",
    "    if 'rolx' in df.columns:\n",
    "        df['rolx_feature'] = df['rolx']  # Example renaming for clarity\n",
    "\n",
    "    # Handle any infinity or NA values that might have been created\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(df.mean())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM0K4ZoqsiKZ"
   },
   "source": [
    "## 3. Feature Selection\n",
    "### 3.1 Advanced Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "35sTZWhIPQvO"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "def advanced_feature_selection(X, y, n_splits=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Performing advanced feature selection using multiple methods.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize TimeSeriesSplit for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # Define a custom scoring function that combines MSE and Spearman correlation\n",
    "    def custom_score(y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        corr, _ = spearmanr(y_true, y_pred)\n",
    "        return -mse * (1 + corr) / 2\n",
    "\n",
    "    custom_scorer = make_scorer(custom_score, greater_is_better=True)\n",
    "\n",
    "    # Recursive Feature Elimination with Cross-Validation (RFECV) using XGBoost\n",
    "    xgb_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    rfecv = RFECV(estimator=xgb_model, step=1, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1, min_features_to_select=1)\n",
    "    rfecv.fit(X, y)\n",
    "    rfecv_features = X.columns[rfecv.support_].tolist()\n",
    "\n",
    "    # LASSO feature selection\n",
    "    lasso = LassoCV(cv=tscv, random_state=42, n_jobs=-1)\n",
    "    lasso.fit(X, y)\n",
    "\n",
    "    lasso_features = X.columns[np.abs(lasso.coef_) > np.percentile(np.abs(lasso.coef_), 75)].tolist()\n",
    "\n",
    "    # Random Forest feature importance\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X, y)\n",
    "\n",
    "    importance_threshold = np.percentile(rf_model.feature_importances_, 75)\n",
    "    rf_features = X.columns[rf_model.feature_importances_ > importance_threshold].tolist()\n",
    "\n",
    "    # Mutual Information feature selection\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "\n",
    "    mi_threshold = np.percentile(mi_scores, 75)\n",
    "    mi_features = X.columns[mi_scores > mi_threshold].tolist()\n",
    "\n",
    "    # Combine all selected features\n",
    "    selected_features = list(set(rfecv_features) | set(lasso_features) | set(rf_features) | set(mi_features))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"RFECV selected {len(rfecv_features)} features\")\n",
    "        print(f\"LASSO selected {len(lasso_features)} features\")\n",
    "        print(f\"Random Forest selected {len(rf_features)} features\")\n",
    "        print(f\"Mutual Information selected {len(mi_features)} features\")\n",
    "        print(f\"Combined selection: {len(selected_features)} features\")\n",
    "\n",
    "\n",
    "        feature_importance = sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)\n",
    "        print(\"\\nTop 10 features by Random Forest importance:\")\n",
    "        for importance, feature in feature_importance[:10]:\n",
    "            print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "\n",
    "    if len(selected_features) < 5:\n",
    "        print(\"Too few features selected. Using top 10 features by Random Forest importance.\")\n",
    "        return [feature for _, feature in feature_importance[:10]]\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "35sTZWhIPQvO"
   },
   "outputs": [],
   "source": [
    "class MultiTargetPredictor:\n",
    "    def __init__(self, models_dict):\n",
    "        self.models = models_dict\n",
    "        self.trained_models = {}\n",
    "        self.scalers = {}\n",
    "    \n",
    "    def fit(self, splits):\n",
    "        \"\"\"Train models for each target variable\"\"\"\n",
    "        for target_name, split_data in splits.items():\n",
    "            print(f\"\\nTraining models for {target_name}...\")\n",
    "            \n",
    "            # Scale target variable\n",
    "            scaler = StandardScaler()\n",
    "            y_train_scaled = scaler.fit_transform(split_data['y_train'].values.reshape(-1, 1)).ravel()\n",
    "            \n",
    "            # Store scaler for prediction\n",
    "            self.scalers[target_name] = scaler\n",
    "            \n",
    "            # Train models for this target\n",
    "            self.trained_models[target_name] = {}\n",
    "            for model_name, model in self.models.items():\n",
    "                print(f\"Training {model_name}...\")\n",
    "                model_copy = clone(model)\n",
    "                model_copy.fit(split_data['X_train'], y_train_scaled)\n",
    "                self.trained_models[target_name][model_name] = model_copy\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions for each target variable\"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        for target_name, models in self.trained_models.items():\n",
    "            target_predictions = {}\n",
    "            \n",
    "            for model_name, model in models.items():\n",
    "                # Make prediction and inverse transform\n",
    "                pred_scaled = model.predict(X_test)\n",
    "                pred = self.scalers[target_name].inverse_transform(pred_scaled.reshape(-1, 1)).ravel()\n",
    "                target_predictions[model_name] = pred\n",
    "            \n",
    "            # Average predictions from different models for this target\n",
    "            predictions[target_name] = np.mean(list(target_predictions.values()), axis=0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWGO1L3siKZ"
   },
   "source": [
    "## 4. Model Development\n",
    "### 4.1 LSTM Model\n",
    "### 4.2 Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OICJ2tUwNKWz",
    "outputId": "49c3b021-11af-4fc8-d7ad-92a2beaa4f4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0160 - val_loss: 0.0095\n",
      "Epoch 2/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0126 - val_loss: 0.0094\n",
      "Epoch 3/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0121 - val_loss: 0.0094\n",
      "Epoch 4/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 5/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0114 - val_loss: 0.0095\n",
      "Epoch 6/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0115 - val_loss: 0.0094\n",
      "Epoch 7/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 8/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0111 - val_loss: 0.0094\n",
      "Epoch 9/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0105 - val_loss: 0.0094\n",
      "Epoch 10/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0121 - val_loss: 0.0094\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0089\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "# Download the dataset\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2020-01-01'\n",
    "df = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
    "\n",
    "# Feature Engineering\n",
    "def engineer_features(df):\n",
    "    # Example: Calculating 7-day Return on Investment (ROI) for price data\n",
    "    df['7d_ROI'] = df['Adj Close'].pct_change(periods=7)\n",
    "    df['7d_ROI'] = df['7d_ROI'].shift(-7)  # Adjust for future price\n",
    "    return df\n",
    "\n",
    "df = engineer_features(df)\n",
    "\n",
    "# Data Cleaning\n",
    "df = df.dropna()\n",
    "\n",
    "# Selecting numeric features only\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Clean the target variable (remove outliers)\n",
    "def clean_target_variable(y):\n",
    "    y = y.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    mean = y.mean()\n",
    "    std = y.std()\n",
    "    y = y[(y > mean - 5*std) & (y < mean + 5*std)]\n",
    "    return y\n",
    "\n",
    "# Clean the target and align X with the cleaned y\n",
    "y_cleaned = clean_target_variable(df['7d_ROI'])\n",
    "X_cleaned = df.drop('7d_ROI', axis=1).loc[y_cleaned.index]  # Align X with the cleaned target\n",
    "\n",
    "# Re-assign the cleaned X and y\n",
    "X = X_cleaned\n",
    "y = y_cleaned\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(len(X) * 0.8)\n",
    "split_date = X.index[train_size]\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Reshape data for LSTM (3D format)\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# LSTM Model Definition\n",
    "def create_lstm_model(input_shape, dropout_rate=0.5, neurons=[512, 256, 128]):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(neurons[0], input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Additional LSTM layers\n",
    "    for units in neurons[1:-1]:\n",
    "        model.add(LSTM(units, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Final LSTM layer\n",
    "    model.add(LSTM(neurons[-1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and train the LSTM model\n",
    "lstm_model = create_lstm_model(input_shape=X_train.shape[1:])\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Optional: Transformer Model (if you'd like to test it as well)\n",
    "def create_transformer_model(input_shape):\n",
    "    config = TimeSeriesTransformerConfig(\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        n_layers=4,\n",
    "        dim_feedforward=256,\n",
    "        dropout=0.1,\n",
    "        activation=\"gelu\",\n",
    "        context_length=input_shape[0],\n",
    "        prediction_length=1\n",
    "    )\n",
    "    model = TimeSeriesTransformerModel(config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-7ioj47HK6-X"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        self.median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.cont_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        self.other_features = X.columns.difference(self.cont_features)\n",
    "        if not self.cont_features.empty:\n",
    "            self.knn_imputer.fit(X[self.cont_features])\n",
    "        if not self.other_features.empty:\n",
    "            self.median_imputer.fit(X[self.other_features])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        if not self.cont_features.empty:\n",
    "            X_cont = pd.DataFrame(\n",
    "                self.knn_imputer.transform(X[self.cont_features]),\n",
    "                columns=self.cont_features,\n",
    "                index=X.index\n",
    "            )\n",
    "            X_transformed[self.cont_features] = X_cont\n",
    "        if not self.other_features.empty:\n",
    "            X_other = pd.DataFrame(\n",
    "                self.median_imputer.transform(X[self.other_features]),\n",
    "                columns=self.other_features,\n",
    "                index=X.index\n",
    "            )\n",
    "            X_transformed[self.other_features] = X_other\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "# Objective function for hyperparameter optimization\n",
    "def objective(trial, X, y, cv, preprocessor):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        pred = model.predict(X_val_processed)\n",
    "        score = mean_squared_error(y_val, pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qxtwBzW1K677"
   },
   "outputs": [],
   "source": [
    "class ExtremeValueHandler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        for column in X.columns:\n",
    "            # mean and standard deviation for the column\n",
    "            mean = X[column].mean()\n",
    "            std = X[column].std()\n",
    "\n",
    "            # Cliping values outside the range [mean - threshold * std, mean + threshold * std]\n",
    "            X_transformed[column] = X[column].clip(\n",
    "                lower=mean - self.threshold * std,\n",
    "                upper=mean + self.threshold * std\n",
    "            )\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKzlUk-5siKb"
   },
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LNVzsyvJK65S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "\n",
    "# Custom transformer to handle extreme values\n",
    "class ExtremeValueHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for column in X_copy.columns:\n",
    "            if X_copy[column].dtype in ['int64', 'float64']:\n",
    "                # Calculating IQR and bounds\n",
    "                Q1 = X_copy[column].quantile(0.25)\n",
    "                Q3 = X_copy[column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - self.threshold * IQR\n",
    "                upper_bound = Q3 + self.threshold * IQR\n",
    "                # Clip values to the bounds\n",
    "                X_copy[column] = X_copy[column].clip(lower_bound, upper_bound)\n",
    "        return X_copy\n",
    "\n",
    "# Custom transformer to filter highly correlated features\n",
    "class CorrelationFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.95):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        self.corr_matrix = X.corr().abs()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        upper = self.corr_matrix.where(np.triu(np.ones(self.corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > self.threshold)]\n",
    "        return X.drop(columns=to_drop)\n",
    "\n",
    "# Wrapper to ensure transformers return DataFrames\n",
    "class DataFrameWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transformer):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        result = self.transformer.transform(X)\n",
    "        if isinstance(result, np.ndarray):\n",
    "            return pd.DataFrame(result, columns=X.columns, index=X.index)\n",
    "        return result\n",
    "\n",
    "def create_pipeline(trial):\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Numeric data preprocessing\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('extreme_value_handler', ExtremeValueHandler(threshold=trial.suggest_float('evh_threshold', 2, 4))),\n",
    "        ('imputer', KNNImputer(n_neighbors=trial.suggest_int('knn_neighbors', 3, 7))),\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('qt', QuantileTransformer(n_quantiles=trial.suggest_int('qt_quantiles', 100, 1000), output_distribution='normal')),\n",
    "    ])\n",
    "\n",
    "    # Categorical data preprocessing\n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ])\n",
    "\n",
    "    # Combining numeric and categorical preprocessors\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "    preprocessor = DataFrameWrapper(preprocessor)\n",
    "\n",
    "    # Feature selection pipeline\n",
    "    feature_selector = Pipeline([\n",
    "        ('variance_threshold', VarianceThreshold(threshold=trial.suggest_float('vt_threshold', 0.005, 0.02))),\n",
    "        ('correlation_filter', CorrelationFilter(threshold=trial.suggest_float('corr_threshold', 0.9, 0.99))),\n",
    "        ('univariate_selection', SelectKBest(score_func=f_regression, k=trial.suggest_int('k_best', 5, len(X.columns)))),  # Fixing the range\n",
    "        ('pca', PCA(n_components=trial.suggest_float('pca_components', 0.8, 0.99))),\n",
    "    ])\n",
    "\n",
    "    # XGBoost model with hyperparameters\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 300),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        learning_rate=trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "        subsample=trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        colsample_bytree=trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        min_child_weight=trial.suggest_int('min_child_weight', 1, 10),\n",
    "        random_state=42\n",
    "    )\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('model', model),\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    pipeline = create_pipeline(trial)\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "def optimize_pipeline(X, y, n_trials=100):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_pipeline = create_pipeline(study.best_trial)\n",
    "    best_pipeline.fit(X, y)\n",
    "\n",
    "    return best_pipeline, study.best_trial\n",
    "\n",
    "# Assuming df is already defined\n",
    "X = df.drop('7d_ROI', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXlCv7vosiKc"
   },
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8PdKR9vK62p",
    "outputId": "fb03cb7d-c2c7-449f-a820-2358ea40bffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X before preprocessing: (1925, 6)\n",
      "Shape of y before preprocessing: (1924,)\n",
      "Any NaN values in X before feature selection: False\n",
      "Any NaN values in y before feature selection: False\n",
      "Mismatch before processing: X has 1925 rows, y has 1924 rows\n",
      "Shape of X after feature selection: (1924, 6)\n",
      "Shape of y after feature selection: (1924,)\n",
      "Shape of X_train: (1539, 6)\n",
      "Shape of X_test: (385, 6)\n",
      "Shape of y_train: (1539,)\n",
      "Shape of y_test: (385,)\n",
      "Shape of X_train after processing: (1539, 6)\n",
      "Shape of X_test after processing: (385, 6)\n",
      "Any infinity values in X_train: False\n",
      "Any infinity values in X_test: False\n",
      "Any NaN values in X_train: False\n",
      "Any NaN values in X_test: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Example function for feature selection (replace with your actual implementation)\n",
    "def advanced_feature_selection(X, y):\n",
    "    # Example feature selection logic (replace with your actual feature selection)\n",
    "    selected_features = X.columns  # For now, return all features as selected\n",
    "    return selected_features\n",
    "\n",
    "# Custom transformer to handle extreme values\n",
    "class ExtremeValueHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_inf=True, replace_large=True, large_threshold=1e300):\n",
    "        self.replace_inf = replace_inf\n",
    "        self.replace_large = replace_large\n",
    "        self.large_threshold = large_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.replace_inf:\n",
    "            X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        if self.replace_large:\n",
    "            X = X.mask(X.abs() > self.large_threshold, np.nan)\n",
    "\n",
    "        return X\n",
    "\n",
    "# Sample dataset (replace with actual data)\n",
    "data = pd.DataFrame({\n",
    "    'feature_1': np.random.rand(1925),\n",
    "    'feature_2': np.random.rand(1925),\n",
    "    'feature_3': np.random.rand(1925),\n",
    "    'feature_4': np.random.rand(1925),\n",
    "    'feature_5': np.random.rand(1925),\n",
    "    'feature_6': np.random.rand(1925)\n",
    "})\n",
    "target = pd.Series(np.random.randint(0, 2, size=1924))  # Target with one less row\n",
    "\n",
    "# Print initial shapes and any NaN values\n",
    "print(f\"Shape of X before preprocessing: {data.shape}\")\n",
    "print(f\"Shape of y before preprocessing: {target.shape}\")\n",
    "print(f\"Any NaN values in X before feature selection: {data.isna().any().any()}\")\n",
    "print(f\"Any NaN values in y before feature selection: {target.isna().any()}\")\n",
    "\n",
    "# Feature selection\n",
    "selected_features = advanced_feature_selection(data, target)\n",
    "\n",
    "# Apply feature selection\n",
    "X = data[selected_features]\n",
    "y = target\n",
    "\n",
    "# Check if there's a mismatch between the number of rows in X and y before preprocessing\n",
    "if X.shape[0] != y.shape[0]:\n",
    "    print(f\"Mismatch before processing: X has {X.shape[0]} rows, y has {y.shape[0]} rows\")\n",
    "    \n",
    "    # Here, let's remove the last row from X (or y) to align them\n",
    "    if X.shape[0] > y.shape[0]:\n",
    "        X = X[:-1]  # Remove the last row from X to match y's length\n",
    "    elif y.shape[0] > X.shape[0]:\n",
    "        y = y[:-1]  # Remove the last row from y to match X's length\n",
    "\n",
    "# Ensure no mismatch between X and y\n",
    "assert X.shape[0] == y.shape[0], \"Mismatch between X and y after fixing\"\n",
    "\n",
    "# Print the updated shapes\n",
    "print(f\"Shape of X after feature selection: {X.shape}\")\n",
    "print(f\"Shape of y after feature selection: {y.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes after train-test split\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('extreme_value_handler', ExtremeValueHandler()),\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply preprocessing steps\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Print results after preprocessing\n",
    "print(f\"Shape of X_train after processing: {X_train.shape}\")\n",
    "print(f\"Shape of X_test after processing: {X_test.shape}\")\n",
    "print(f\"Any infinity values in X_train: {np.isinf(X_train).any().any()}\")\n",
    "print(f\"Any infinity values in X_test: {np.isinf(X_test).any().any()}\")\n",
    "print(f\"Any NaN values in X_train: {np.isnan(X_train).any().any()}\")\n",
    "print(f\"Any NaN values in X_test: {np.isnan(X_test).any().any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qZs3L5zK6z3",
    "outputId": "df61bd90-a58f-4983-f9a1-17ae455f88ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-16 13:23:46,105] A new study created in memory with name: no-name-94f337c9-5ee9-4ab5-87cb-ede9b3f42dc0\n",
      "[I 2024-11-16 13:23:48,173] Trial 0 finished with value: 0.3138118032318672 and parameters: {'n_estimators': 628, 'max_depth': 6, 'learning_rate': 0.16402383878834642, 'subsample': 0.7744267378521785, 'subsample_freq': 8}. Best is trial 0 with value: 0.3138118032318672.\n",
      "[I 2024-11-16 13:23:51,768] Trial 1 finished with value: 0.29181660363737644 and parameters: {'n_estimators': 458, 'max_depth': 9, 'learning_rate': 0.05602268701431596, 'subsample': 0.9068795475289804, 'subsample_freq': 1}. Best is trial 1 with value: 0.29181660363737644.\n",
      "[I 2024-11-16 13:23:52,564] Trial 2 finished with value: 0.5078785674316484 and parameters: {'n_estimators': 410, 'max_depth': 5, 'learning_rate': 0.8801901554156614, 'subsample': 0.7429883888402786, 'subsample_freq': 4}. Best is trial 1 with value: 0.29181660363737644.\n",
      "[I 2024-11-16 13:23:56,002] Trial 3 finished with value: 0.3070029781401917 and parameters: {'n_estimators': 855, 'max_depth': 6, 'learning_rate': 0.06940055114769514, 'subsample': 0.9396582055053317, 'subsample_freq': 4}. Best is trial 1 with value: 0.29181660363737644.\n",
      "[I 2024-11-16 13:23:57,437] Trial 4 finished with value: 0.2524291696976722 and parameters: {'n_estimators': 527, 'max_depth': 5, 'learning_rate': 0.0013020059949776365, 'subsample': 0.7975425375799657, 'subsample_freq': 2}. Best is trial 4 with value: 0.2524291696976722.\n",
      "[I 2024-11-16 13:23:58,519] Trial 5 finished with value: 0.2514532408618025 and parameters: {'n_estimators': 169, 'max_depth': 9, 'learning_rate': 0.0014498841674352714, 'subsample': 0.620906921405288, 'subsample_freq': 2}. Best is trial 5 with value: 0.2514532408618025.\n",
      "[I 2024-11-16 13:24:00,134] Trial 6 finished with value: 0.3238585096198684 and parameters: {'n_estimators': 764, 'max_depth': 7, 'learning_rate': 0.27168723747294893, 'subsample': 0.7732457929999472, 'subsample_freq': 9}. Best is trial 5 with value: 0.2514532408618025.\n",
      "[I 2024-11-16 13:24:01,486] Trial 7 finished with value: 0.2668746612648928 and parameters: {'n_estimators': 930, 'max_depth': 3, 'learning_rate': 0.008350423271673641, 'subsample': 0.7796578431331515, 'subsample_freq': 2}. Best is trial 5 with value: 0.2514532408618025.\n",
      "[I 2024-11-16 13:24:02,219] Trial 8 finished with value: 0.2940810408969684 and parameters: {'n_estimators': 175, 'max_depth': 6, 'learning_rate': 0.07710548806501116, 'subsample': 0.8230811059918415, 'subsample_freq': 2}. Best is trial 5 with value: 0.2514532408618025.\n",
      "[I 2024-11-16 13:24:03,755] Trial 9 finished with value: 0.29746427744200565 and parameters: {'n_estimators': 226, 'max_depth': 8, 'learning_rate': 0.05826186910232888, 'subsample': 0.6393614442353989, 'subsample_freq': 1}. Best is trial 5 with value: 0.2514532408618025.\n",
      "[I 2024-11-16 13:24:04,625] Trial 10 finished with value: 0.250941471735551 and parameters: {'n_estimators': 107, 'max_depth': 10, 'learning_rate': 0.0012964161831596449, 'subsample': 0.6051643667502787, 'subsample_freq': 6}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:06,786] Trial 11 finished with value: 0.25193743427386156 and parameters: {'n_estimators': 289, 'max_depth': 10, 'learning_rate': 0.0011128493593759805, 'subsample': 0.6086076070631973, 'subsample_freq': 7}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:07,622] Trial 12 finished with value: 0.25388078730779995 and parameters: {'n_estimators': 106, 'max_depth': 10, 'learning_rate': 0.005481400658709589, 'subsample': 0.6720712532462196, 'subsample_freq': 6}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:09,727] Trial 13 finished with value: 0.2603682427187525 and parameters: {'n_estimators': 311, 'max_depth': 9, 'learning_rate': 0.0044175846234913, 'subsample': 0.6930477325346567, 'subsample_freq': 5}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:10,371] Trial 14 finished with value: 0.26311973130460303 and parameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.016670733954571113, 'subsample': 0.604853550275846, 'subsample_freq': 10}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:12,953] Trial 15 finished with value: 0.25458513214089995 and parameters: {'n_estimators': 330, 'max_depth': 10, 'learning_rate': 0.002266854729926097, 'subsample': 0.6996135737294042, 'subsample_freq': 4}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:17,976] Trial 16 finished with value: 0.29620447632394764 and parameters: {'n_estimators': 650, 'max_depth': 9, 'learning_rate': 0.01750302542570077, 'subsample': 0.9979934667356043, 'subsample_freq': 6}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:19,214] Trial 17 finished with value: 0.2535095825008818 and parameters: {'n_estimators': 213, 'max_depth': 8, 'learning_rate': 0.002816739964472346, 'subsample': 0.652715228005077, 'subsample_freq': 7}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:21,961] Trial 18 finished with value: 0.27524939252417235 and parameters: {'n_estimators': 393, 'max_depth': 9, 'learning_rate': 0.01035663683928282, 'subsample': 0.8557959650212784, 'subsample_freq': 3}. Best is trial 10 with value: 0.250941471735551.\n",
      "[I 2024-11-16 13:24:22,139] Trial 19 finished with value: 0.2507088823538337 and parameters: {'n_estimators': 102, 'max_depth': 3, 'learning_rate': 0.0029257535211633756, 'subsample': 0.6046684908479463, 'subsample_freq': 5}. Best is trial 19 with value: 0.2507088823538337.\n",
      "[I 2024-11-16 13:24:22,901] Trial 20 finished with value: 0.2531732856669932 and parameters: {'n_estimators': 526, 'max_depth': 3, 'learning_rate': 0.0028167143290297996, 'subsample': 0.7360413999733668, 'subsample_freq': 5}. Best is trial 19 with value: 0.2507088823538337.\n",
      "[I 2024-11-16 13:24:23,127] Trial 21 finished with value: 0.25046904596871256 and parameters: {'n_estimators': 102, 'max_depth': 4, 'learning_rate': 0.0010180048958130965, 'subsample': 0.6005519791845921, 'subsample_freq': 7}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:23,358] Trial 22 finished with value: 0.25068088206524936 and parameters: {'n_estimators': 101, 'max_depth': 4, 'learning_rate': 0.0019679216887475794, 'subsample': 0.6562533914941769, 'subsample_freq': 7}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:23,844] Trial 23 finished with value: 0.25309097417531967 and parameters: {'n_estimators': 240, 'max_depth': 4, 'learning_rate': 0.004146244821412001, 'subsample': 0.6589629733264104, 'subsample_freq': 8}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:24,403] Trial 24 finished with value: 0.2513074170018161 and parameters: {'n_estimators': 279, 'max_depth': 4, 'learning_rate': 0.0020190521910346034, 'subsample': 0.7083386229350193, 'subsample_freq': 7}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:24,748] Trial 25 finished with value: 0.25424440148535765 and parameters: {'n_estimators': 167, 'max_depth': 4, 'learning_rate': 0.008109042289300481, 'subsample': 0.6387667724338227, 'subsample_freq': 8}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:25,289] Trial 26 finished with value: 0.2528129528109447 and parameters: {'n_estimators': 367, 'max_depth': 3, 'learning_rate': 0.0034158699542461562, 'subsample': 0.6678174580007905, 'subsample_freq': 9}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:25,775] Trial 27 finished with value: 0.267249065314927 and parameters: {'n_estimators': 165, 'max_depth': 5, 'learning_rate': 0.025093793701781608, 'subsample': 0.7364218967581295, 'subsample_freq': 5}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:26,265] Trial 28 finished with value: 0.2547645539359852 and parameters: {'n_estimators': 241, 'max_depth': 4, 'learning_rate': 0.006476785425433915, 'subsample': 0.627277554787714, 'subsample_freq': 7}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:27,174] Trial 29 finished with value: 0.2525225835527934 and parameters: {'n_estimators': 626, 'max_depth': 3, 'learning_rate': 0.0019238518471157264, 'subsample': 0.6818692625224431, 'subsample_freq': 9}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:28,565] Trial 30 finished with value: 0.25149345978305915 and parameters: {'n_estimators': 471, 'max_depth': 5, 'learning_rate': 0.0010398650567483457, 'subsample': 0.6391524268204166, 'subsample_freq': 8}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:28,825] Trial 31 finished with value: 0.2506285842511641 and parameters: {'n_estimators': 115, 'max_depth': 4, 'learning_rate': 0.0017123384892497727, 'subsample': 0.6044207815158951, 'subsample_freq': 6}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:29,073] Trial 32 finished with value: 0.25065721975433064 and parameters: {'n_estimators': 111, 'max_depth': 4, 'learning_rate': 0.0018057875741973371, 'subsample': 0.6033781673460513, 'subsample_freq': 6}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:29,442] Trial 33 finished with value: 0.2508009498994387 and parameters: {'n_estimators': 174, 'max_depth': 4, 'learning_rate': 0.0015595488771362382, 'subsample': 0.6000294012116724, 'subsample_freq': 6}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:30,205] Trial 34 finished with value: 0.34411666550953696 and parameters: {'n_estimators': 220, 'max_depth': 5, 'learning_rate': 0.25910322811421865, 'subsample': 0.6504462314611477, 'subsample_freq': 7}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:30,518] Trial 35 finished with value: 0.25055564502246835 and parameters: {'n_estimators': 147, 'max_depth': 4, 'learning_rate': 0.0016836994444796066, 'subsample': 0.7180363496856611, 'subsample_freq': 6}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:31,005] Trial 36 finished with value: 0.2595061859682387 and parameters: {'n_estimators': 154, 'max_depth': 5, 'learning_rate': 0.013179776705506189, 'subsample': 0.7165168049147279, 'subsample_freq': 4}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:32,794] Trial 37 finished with value: 0.25173266851525267 and parameters: {'n_estimators': 433, 'max_depth': 6, 'learning_rate': 0.0010240025869428168, 'subsample': 0.8377325056171041, 'subsample_freq': 6}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:33,460] Trial 38 finished with value: 0.316120615223049 and parameters: {'n_estimators': 328, 'max_depth': 4, 'learning_rate': 0.11597864147143053, 'subsample': 0.7619389939160255, 'subsample_freq': 5}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:34,121] Trial 39 finished with value: 0.3723691059870983 and parameters: {'n_estimators': 267, 'max_depth': 7, 'learning_rate': 0.6137402224012759, 'subsample': 0.8939175842414847, 'subsample_freq': 4}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:36,522] Trial 40 finished with value: 0.2657797882576117 and parameters: {'n_estimators': 829, 'max_depth': 5, 'learning_rate': 0.0046986898002039194, 'subsample': 0.6257000901092172, 'subsample_freq': 6}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:36,829] Trial 41 finished with value: 0.2506679000997176 and parameters: {'n_estimators': 144, 'max_depth': 4, 'learning_rate': 0.0019343252539880899, 'subsample': 0.6341346736448185, 'subsample_freq': 7}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:37,067] Trial 42 finished with value: 0.2505487817291908 and parameters: {'n_estimators': 143, 'max_depth': 3, 'learning_rate': 0.0016486230852820187, 'subsample': 0.6233184776729759, 'subsample_freq': 8}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:37,420] Trial 43 finished with value: 0.2506362535013351 and parameters: {'n_estimators': 215, 'max_depth': 3, 'learning_rate': 0.0014723708902673938, 'subsample': 0.619720426928358, 'subsample_freq': 8}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:37,741] Trial 44 finished with value: 0.25050804919273145 and parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.00127741082654341, 'subsample': 0.6743589602315101, 'subsample_freq': 9}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:38,053] Trial 45 finished with value: 0.250559387318268 and parameters: {'n_estimators': 199, 'max_depth': 3, 'learning_rate': 0.0013728156078955413, 'subsample': 0.6721935648737849, 'subsample_freq': 10}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:39,470] Trial 46 finished with value: 0.25249622091523743 and parameters: {'n_estimators': 978, 'max_depth': 3, 'learning_rate': 0.0012748842342102924, 'subsample': 0.6798832019508765, 'subsample_freq': 10}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:39,782] Trial 47 finished with value: 0.25113709225743847 and parameters: {'n_estimators': 196, 'max_depth': 3, 'learning_rate': 0.003419098647921349, 'subsample': 0.7121601234809455, 'subsample_freq': 9}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:40,201] Trial 48 finished with value: 0.25049966692754916 and parameters: {'n_estimators': 269, 'max_depth': 3, 'learning_rate': 0.0010010168922343315, 'subsample': 0.7569921540887876, 'subsample_freq': 10}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:40,607] Trial 49 finished with value: 0.2511446946290661 and parameters: {'n_estimators': 260, 'max_depth': 3, 'learning_rate': 0.0024724696757900348, 'subsample': 0.7605596173176103, 'subsample_freq': 9}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:41,058] Trial 50 finished with value: 0.2505734594049437 and parameters: {'n_estimators': 299, 'max_depth': 3, 'learning_rate': 0.001018220855029239, 'subsample': 0.719619675877938, 'subsample_freq': 10}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:41,370] Trial 51 finished with value: 0.2505142212150948 and parameters: {'n_estimators': 194, 'max_depth': 3, 'learning_rate': 0.0013575185104763652, 'subsample': 0.7895969232998514, 'subsample_freq': 10}. Best is trial 21 with value: 0.25046904596871256.\n",
      "[I 2024-11-16 13:24:41,609] Trial 52 finished with value: 0.25041546676606 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.0013590761405302782, 'subsample': 0.7967430746660595, 'subsample_freq': 9}. Best is trial 52 with value: 0.25041546676606.\n",
      "[I 2024-11-16 13:24:41,849] Trial 53 finished with value: 0.2503824415428172 and parameters: {'n_estimators': 142, 'max_depth': 3, 'learning_rate': 0.0012635865590897592, 'subsample': 0.7782827382265326, 'subsample_freq': 10}. Best is trial 53 with value: 0.2503824415428172.\n",
      "[I 2024-11-16 13:24:42,152] Trial 54 finished with value: 0.2677549244535162 and parameters: {'n_estimators': 191, 'max_depth': 3, 'learning_rate': 0.04400990420725682, 'subsample': 0.8007055996414932, 'subsample_freq': 10}. Best is trial 53 with value: 0.2503824415428172.\n",
      "[I 2024-11-16 13:24:42,692] Trial 55 finished with value: 0.25078464419605684 and parameters: {'n_estimators': 360, 'max_depth': 3, 'learning_rate': 0.0012936370430196315, 'subsample': 0.7888654702610803, 'subsample_freq': 9}. Best is trial 53 with value: 0.2503824415428172.\n",
      "[I 2024-11-16 13:24:43,066] Trial 56 finished with value: 0.25110774342376624 and parameters: {'n_estimators': 244, 'max_depth': 3, 'learning_rate': 0.0024335280150612542, 'subsample': 0.8090688672641584, 'subsample_freq': 10}. Best is trial 53 with value: 0.2503824415428172.\n",
      "[I 2024-11-16 13:24:43,510] Trial 57 finished with value: 0.2520597682661962 and parameters: {'n_estimators': 286, 'max_depth': 3, 'learning_rate': 0.0036348566744996997, 'subsample': 0.8262991077123241, 'subsample_freq': 9}. Best is trial 53 with value: 0.2503824415428172.\n",
      "[I 2024-11-16 13:24:43,810] Trial 58 finished with value: 0.25194662649703325 and parameters: {'n_estimators': 139, 'max_depth': 4, 'learning_rate': 0.005504107558426029, 'subsample': 0.771001227693989, 'subsample_freq': 10}. Best is trial 53 with value: 0.2503824415428172.\n",
      "[I 2024-11-16 13:24:44,109] Trial 59 finished with value: 0.2503649350223506 and parameters: {'n_estimators': 190, 'max_depth': 3, 'learning_rate': 0.0010156485270356086, 'subsample': 0.8737787450476493, 'subsample_freq': 10}. Best is trial 59 with value: 0.2503649350223506.\n",
      "[I 2024-11-16 13:24:44,629] Trial 60 finished with value: 0.25161859606052905 and parameters: {'n_estimators': 344, 'max_depth': 3, 'learning_rate': 0.0024381120228198897, 'subsample': 0.9181162314164376, 'subsample_freq': 9}. Best is trial 59 with value: 0.2503649350223506.\n",
      "[I 2024-11-16 13:24:44,932] Trial 61 finished with value: 0.2503918144152697 and parameters: {'n_estimators': 192, 'max_depth': 3, 'learning_rate': 0.001229704464670619, 'subsample': 0.8640436690669901, 'subsample_freq': 10}. Best is trial 59 with value: 0.2503649350223506.\n",
      "[I 2024-11-16 13:24:45,163] Trial 62 finished with value: 0.25031154289019286 and parameters: {'n_estimators': 135, 'max_depth': 3, 'learning_rate': 0.0010030953230415016, 'subsample': 0.8725103473331555, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:45,460] Trial 63 finished with value: 0.2503570598793681 and parameters: {'n_estimators': 122, 'max_depth': 4, 'learning_rate': 0.0010268267540822365, 'subsample': 0.866501415406776, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:45,778] Trial 64 finished with value: 0.2504166994486174 and parameters: {'n_estimators': 136, 'max_depth': 4, 'learning_rate': 0.001247447099649497, 'subsample': 0.873349596220929, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:46,145] Trial 65 finished with value: 0.2508247363609106 and parameters: {'n_estimators': 128, 'max_depth': 4, 'learning_rate': 0.00293277951444261, 'subsample': 0.8742707446747082, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:48,443] Trial 66 finished with value: 0.2557816447230012 and parameters: {'n_estimators': 695, 'max_depth': 5, 'learning_rate': 0.002142795553139502, 'subsample': 0.859101839683813, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:48,860] Trial 67 finished with value: 0.2503969674047224 and parameters: {'n_estimators': 171, 'max_depth': 4, 'learning_rate': 0.0012069732055171292, 'subsample': 0.9510706253761263, 'subsample_freq': 9}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:49,146] Trial 68 finished with value: 0.2504580942610582 and parameters: {'n_estimators': 176, 'max_depth': 3, 'learning_rate': 0.0016131777461573715, 'subsample': 0.9508404217343848, 'subsample_freq': 9}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:49,633] Trial 69 finished with value: 0.2506094327494999 and parameters: {'n_estimators': 235, 'max_depth': 4, 'learning_rate': 0.0012047863977643974, 'subsample': 0.9596002845048457, 'subsample_freq': 9}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:50,459] Trial 70 finished with value: 0.25244332545390347 and parameters: {'n_estimators': 167, 'max_depth': 7, 'learning_rate': 0.0029700747450384854, 'subsample': 0.9175624971254548, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:50,748] Trial 71 finished with value: 0.2504637344243402 and parameters: {'n_estimators': 132, 'max_depth': 4, 'learning_rate': 0.0014813795327863401, 'subsample': 0.88901359259494, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:51,030] Trial 72 finished with value: 0.2503247783609931 and parameters: {'n_estimators': 127, 'max_depth': 4, 'learning_rate': 0.001173380140706153, 'subsample': 0.8546828196935294, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:51,338] Trial 73 finished with value: 0.2506561501539399 and parameters: {'n_estimators': 101, 'max_depth': 5, 'learning_rate': 0.002202357164937408, 'subsample': 0.8425714615246326, 'subsample_freq': 9}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:51,627] Trial 74 finished with value: 0.2504213167380353 and parameters: {'n_estimators': 176, 'max_depth': 3, 'learning_rate': 0.00119084604268409, 'subsample': 0.8182429022408114, 'subsample_freq': 10}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:52,084] Trial 75 finished with value: 0.25078776676639836 and parameters: {'n_estimators': 222, 'max_depth': 4, 'learning_rate': 0.0017914504981223294, 'subsample': 0.8529487458118049, 'subsample_freq': 1}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:52,423] Trial 76 finished with value: 0.25052980005077974 and parameters: {'n_estimators': 158, 'max_depth': 4, 'learning_rate': 0.0015404224579589896, 'subsample': 0.8992635395631593, 'subsample_freq': 9}. Best is trial 62 with value: 0.25031154289019286.\n",
      "[I 2024-11-16 13:24:52,638] Trial 77 finished with value: 0.25029965128023773 and parameters: {'n_estimators': 127, 'max_depth': 3, 'learning_rate': 0.0010003423492635968, 'subsample': 0.9828990867678911, 'subsample_freq': 10}. Best is trial 77 with value: 0.25029965128023773.\n",
      "[I 2024-11-16 13:24:52,904] Trial 78 finished with value: 0.250184913694273 and parameters: {'n_estimators': 123, 'max_depth': 4, 'learning_rate': 0.0010558953615934308, 'subsample': 0.9923864208129382, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:53,390] Trial 79 finished with value: 0.2503197322011262 and parameters: {'n_estimators': 122, 'max_depth': 6, 'learning_rate': 0.0010050235489959909, 'subsample': 0.9867431067735251, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:53,864] Trial 80 finished with value: 0.25039876339433886 and parameters: {'n_estimators': 121, 'max_depth': 6, 'learning_rate': 0.0010386991630244428, 'subsample': 0.9917987793579524, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:54,653] Trial 81 finished with value: 0.2508313452726624 and parameters: {'n_estimators': 125, 'max_depth': 8, 'learning_rate': 0.0019275023521270434, 'subsample': 0.9882965515112702, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:55,069] Trial 82 finished with value: 0.2502756648145756 and parameters: {'n_estimators': 104, 'max_depth': 6, 'learning_rate': 0.0011083085132542602, 'subsample': 0.9778242093578753, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:55,489] Trial 83 finished with value: 0.2504413821154526 and parameters: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.0015745662729109678, 'subsample': 0.9768501043170059, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:56,191] Trial 84 finished with value: 0.25060957083275637 and parameters: {'n_estimators': 123, 'max_depth': 7, 'learning_rate': 0.0018153106027644684, 'subsample': 0.9730514481318157, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:56,846] Trial 85 finished with value: 0.25053793744030267 and parameters: {'n_estimators': 151, 'max_depth': 6, 'learning_rate': 0.0010278538555575504, 'subsample': 0.9238149986133234, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:59,295] Trial 86 finished with value: 0.25308486161379057 and parameters: {'n_estimators': 559, 'max_depth': 6, 'learning_rate': 0.0011300706985371578, 'subsample': 0.9343048756552993, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:24:59,878] Trial 87 finished with value: 0.25052271840821005 and parameters: {'n_estimators': 157, 'max_depth': 5, 'learning_rate': 0.0014686665175240525, 'subsample': 0.9743464743453303, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:01,053] Trial 88 finished with value: 0.2525714290986572 and parameters: {'n_estimators': 213, 'max_depth': 7, 'learning_rate': 0.0021804312874248886, 'subsample': 0.9963042857931563, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:01,409] Trial 89 finished with value: 0.25093288542381326 and parameters: {'n_estimators': 122, 'max_depth': 5, 'learning_rate': 0.002614320688050581, 'subsample': 0.98412021592942, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:02,334] Trial 90 finished with value: 0.2511413000797157 and parameters: {'n_estimators': 248, 'max_depth': 6, 'learning_rate': 0.0011540265848207136, 'subsample': 0.9598549404433329, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:02,642] Trial 91 finished with value: 0.250482708063587 and parameters: {'n_estimators': 190, 'max_depth': 3, 'learning_rate': 0.0014190048880063674, 'subsample': 0.8775560007055981, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:02,946] Trial 92 finished with value: 0.25034463157805664 and parameters: {'n_estimators': 186, 'max_depth': 3, 'learning_rate': 0.0011814961594104332, 'subsample': 0.8581905493671479, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:03,130] Trial 93 finished with value: 0.2503111685401306 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0010217281943755926, 'subsample': 0.8437535464003456, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:03,390] Trial 94 finished with value: 0.45832162788918634 and parameters: {'n_estimators': 111, 'max_depth': 4, 'learning_rate': 0.9635308682112327, 'subsample': 0.8428087153287489, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:03,777] Trial 95 finished with value: 0.2503616911495996 and parameters: {'n_estimators': 178, 'max_depth': 3, 'learning_rate': 0.0010172737208895913, 'subsample': 0.8273631143508686, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:05,119] Trial 96 finished with value: 0.251603919859405 and parameters: {'n_estimators': 163, 'max_depth': 9, 'learning_rate': 0.0017518665888441266, 'subsample': 0.831932585421578, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:05,802] Trial 97 finished with value: 0.35740247537170083 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.4636967766826385, 'subsample': 0.8167460566113919, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:06,106] Trial 98 finished with value: 0.25026814194532665 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.0010114818056498615, 'subsample': 0.8538688449715509, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:06,600] Trial 99 finished with value: 0.3077996629730754 and parameters: {'n_estimators': 135, 'max_depth': 5, 'learning_rate': 0.14140788158692066, 'subsample': 0.8827451611906094, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:07,106] Trial 100 finished with value: 0.25055238461768553 and parameters: {'n_estimators': 209, 'max_depth': 4, 'learning_rate': 0.0013821430545092267, 'subsample': 0.9995311815304173, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:07,335] Trial 101 finished with value: 0.2502862015280257 and parameters: {'n_estimators': 123, 'max_depth': 3, 'learning_rate': 0.0011442389104471345, 'subsample': 0.8480102295247054, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:07,582] Trial 102 finished with value: 0.2502875092324369 and parameters: {'n_estimators': 121, 'max_depth': 3, 'learning_rate': 0.0011466015264692743, 'subsample': 0.8483219835683609, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:07,867] Trial 103 finished with value: 0.2503659535636302 and parameters: {'n_estimators': 153, 'max_depth': 3, 'learning_rate': 0.0016256470001315955, 'subsample': 0.8446284296556875, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:08,163] Trial 104 finished with value: 0.25027234724486036 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.001172084227830427, 'subsample': 0.8533445624804508, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:08,428] Trial 105 finished with value: 0.25030016604867955 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.0020539176967378334, 'subsample': 0.8509916124066836, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:08,614] Trial 106 finished with value: 0.2503420111567066 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0019413365958903836, 'subsample': 0.8347787852406363, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:08,872] Trial 107 finished with value: 0.26004403658502906 and parameters: {'n_estimators': 148, 'max_depth': 3, 'learning_rate': 0.025886290825325224, 'subsample': 0.9628937565153006, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:09,086] Trial 108 finished with value: 0.25028441306920335 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.001500234643653604, 'subsample': 0.8496976654634301, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:09,379] Trial 109 finished with value: 0.25036934478217054 and parameters: {'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.0014364997840624267, 'subsample': 0.8487420705505616, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:10,794] Trial 110 finished with value: 0.2543255465151536 and parameters: {'n_estimators': 885, 'max_depth': 3, 'learning_rate': 0.0022073642190174165, 'subsample': 0.8104443741277262, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:11,009] Trial 111 finished with value: 0.25033342538181297 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.001349713518755128, 'subsample': 0.8670007326679073, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:11,273] Trial 112 finished with value: 0.2503994015273728 and parameters: {'n_estimators': 141, 'max_depth': 3, 'learning_rate': 0.00165857404786345, 'subsample': 0.9667887341231278, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:11,489] Trial 113 finished with value: 0.25029800301600647 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.0011434583853266846, 'subsample': 0.984679346350308, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:11,735] Trial 114 finished with value: 0.2503161657112704 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.0011789339674808197, 'subsample': 0.8871132731636696, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:11,995] Trial 115 finished with value: 0.25033719494589735 and parameters: {'n_estimators': 138, 'max_depth': 3, 'learning_rate': 0.001399191370188037, 'subsample': 0.9513428541299173, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:12,284] Trial 116 finished with value: 0.2779441854039508 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.08424640604479458, 'subsample': 0.8628813724040664, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:12,580] Trial 117 finished with value: 0.25047736080314964 and parameters: {'n_estimators': 175, 'max_depth': 3, 'learning_rate': 0.0019611137743575733, 'subsample': 0.8495799826397005, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:12,769] Trial 118 finished with value: 0.2502997002074455 and parameters: {'n_estimators': 102, 'max_depth': 3, 'learning_rate': 0.001581302051574161, 'subsample': 0.8373064413069754, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:13,491] Trial 119 finished with value: 0.25133837937881937 and parameters: {'n_estimators': 470, 'max_depth': 3, 'learning_rate': 0.0015964583545179517, 'subsample': 0.9043601772874735, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:13,709] Trial 120 finished with value: 0.2526371897259234 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.009886883140869428, 'subsample': 0.980113457908304, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:13,895] Trial 121 finished with value: 0.2503041543606773 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.001129181756460692, 'subsample': 0.8362713899779127, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:14,077] Trial 122 finished with value: 0.25031788514914144 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0011590668883441498, 'subsample': 0.8226294396255567, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:14,331] Trial 123 finished with value: 0.2503379999081652 and parameters: {'n_estimators': 135, 'max_depth': 3, 'learning_rate': 0.001345520768233533, 'subsample': 0.8338846245513749, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:14,582] Trial 124 finished with value: 0.25033216422796306 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.0017574551715107225, 'subsample': 0.8388791257005455, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:14,848] Trial 125 finished with value: 0.2504016324782845 and parameters: {'n_estimators': 154, 'max_depth': 3, 'learning_rate': 0.0011734445571403471, 'subsample': 0.8072810302157456, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:15,950] Trial 126 finished with value: 0.2520214040074296 and parameters: {'n_estimators': 740, 'max_depth': 3, 'learning_rate': 0.0015298903072313015, 'subsample': 0.8507427844997717, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:16,136] Trial 127 finished with value: 0.2503084740044872 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0013089213873660683, 'subsample': 0.8315004614512407, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:16,394] Trial 128 finished with value: 0.2504110668005694 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.0020907774554433586, 'subsample': 0.8284354375103083, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:16,655] Trial 129 finished with value: 0.25078256727130777 and parameters: {'n_estimators': 155, 'max_depth': 3, 'learning_rate': 0.0026344018383457073, 'subsample': 0.9696795243375261, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:17,268] Trial 130 finished with value: 0.25282085686752065 and parameters: {'n_estimators': 402, 'max_depth': 3, 'learning_rate': 0.003221864672071901, 'subsample': 0.819697086136571, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:17,489] Trial 131 finished with value: 0.25030754910302894 and parameters: {'n_estimators': 112, 'max_depth': 3, 'learning_rate': 0.0012485675756833186, 'subsample': 0.8384152656492276, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:17,701] Trial 132 finished with value: 0.2503005568037698 and parameters: {'n_estimators': 120, 'max_depth': 3, 'learning_rate': 0.0013326139236308052, 'subsample': 0.8578060957261655, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:17,913] Trial 133 finished with value: 0.25029529001382167 and parameters: {'n_estimators': 123, 'max_depth': 3, 'learning_rate': 0.001164161504341105, 'subsample': 0.8574674883759109, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:18,142] Trial 134 finished with value: 0.25031651298737795 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.0015226536367624738, 'subsample': 0.8589677424905341, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:18,425] Trial 135 finished with value: 0.25034011324245037 and parameters: {'n_estimators': 170, 'max_depth': 3, 'learning_rate': 0.0011439490317375486, 'subsample': 0.8649414439014407, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:18,669] Trial 136 finished with value: 0.2503176482264055 and parameters: {'n_estimators': 144, 'max_depth': 3, 'learning_rate': 0.0017807599947759086, 'subsample': 0.8540315561383309, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:19,117] Trial 137 finished with value: 0.25067964762229256 and parameters: {'n_estimators': 207, 'max_depth': 4, 'learning_rate': 0.0013741198050316924, 'subsample': 0.8818083910565025, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:19,334] Trial 138 finished with value: 0.2503069217412017 and parameters: {'n_estimators': 122, 'max_depth': 3, 'learning_rate': 0.0011160469265426517, 'subsample': 0.896486545034506, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:19,718] Trial 139 finished with value: 0.2504660816361632 and parameters: {'n_estimators': 182, 'max_depth': 3, 'learning_rate': 0.0016578984077408212, 'subsample': 0.8692305268896735, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:20,003] Trial 140 finished with value: 0.2502986355767677 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.001308976596122722, 'subsample': 0.8472099330222866, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:20,248] Trial 141 finished with value: 0.2503022123857434 and parameters: {'n_estimators': 143, 'max_depth': 3, 'learning_rate': 0.0013424094168586515, 'subsample': 0.8469777297313253, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:20,498] Trial 142 finished with value: 0.25032089457054607 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.001366428814448827, 'subsample': 0.8475809160892848, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:21,298] Trial 143 finished with value: 0.2517694840587432 and parameters: {'n_estimators': 515, 'max_depth': 3, 'learning_rate': 0.0015134951219176011, 'subsample': 0.9878783987462012, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:21,524] Trial 144 finished with value: 0.2503989019631228 and parameters: {'n_estimators': 128, 'max_depth': 3, 'learning_rate': 0.00229033894432999, 'subsample': 0.8580873263614504, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:21,867] Trial 145 finished with value: 0.2591496634651679 and parameters: {'n_estimators': 157, 'max_depth': 4, 'learning_rate': 0.017747855720549773, 'subsample': 0.8762204941194509, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:22,159] Trial 146 finished with value: 0.2503566689732003 and parameters: {'n_estimators': 178, 'max_depth': 3, 'learning_rate': 0.0012230063020878593, 'subsample': 0.8451792972540052, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:22,384] Trial 147 finished with value: 0.25042436034271803 and parameters: {'n_estimators': 129, 'max_depth': 3, 'learning_rate': 0.0019188578604151009, 'subsample': 0.9796918464957938, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:22,704] Trial 148 finished with value: 0.2503527826663978 and parameters: {'n_estimators': 195, 'max_depth': 3, 'learning_rate': 0.0010311581833976448, 'subsample': 0.9396017354216513, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:22,957] Trial 149 finished with value: 0.2502937341889465 and parameters: {'n_estimators': 149, 'max_depth': 3, 'learning_rate': 0.0013059996422320416, 'subsample': 0.8560767991001748, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:23,233] Trial 150 finished with value: 0.2503968333026951 and parameters: {'n_estimators': 118, 'max_depth': 4, 'learning_rate': 0.0016996844895634119, 'subsample': 0.8601574584602878, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:23,489] Trial 151 finished with value: 0.2502803503131709 and parameters: {'n_estimators': 148, 'max_depth': 3, 'learning_rate': 0.0010024375430028488, 'subsample': 0.8525672943039827, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:23,751] Trial 152 finished with value: 0.25032100618746506 and parameters: {'n_estimators': 151, 'max_depth': 3, 'learning_rate': 0.0010013451969589503, 'subsample': 0.8688643096115743, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:24,024] Trial 153 finished with value: 0.2503251571673816 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.0012496829890116356, 'subsample': 0.8547725566297182, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:24,233] Trial 154 finished with value: 0.2503003884011766 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.001118546407683102, 'subsample': 0.8857644400879325, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:24,439] Trial 155 finished with value: 0.25031624436362765 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.001484802099786472, 'subsample': 0.9982852576806709, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:24,669] Trial 156 finished with value: 0.25033270341165414 and parameters: {'n_estimators': 135, 'max_depth': 3, 'learning_rate': 0.0011386600568894837, 'subsample': 0.8833028364832431, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:24,947] Trial 157 finished with value: 0.25031255491378757 and parameters: {'n_estimators': 170, 'max_depth': 3, 'learning_rate': 0.0011112893585850205, 'subsample': 0.837935210929345, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:25,198] Trial 158 finished with value: 0.25037059645151916 and parameters: {'n_estimators': 147, 'max_depth': 3, 'learning_rate': 0.0015310316156413046, 'subsample': 0.8685942097114323, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:25,552] Trial 159 finished with value: 0.25041532150975165 and parameters: {'n_estimators': 223, 'max_depth': 3, 'learning_rate': 0.0010017477214860461, 'subsample': 0.991849373226393, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:26,520] Trial 160 finished with value: 0.251381171447907 and parameters: {'n_estimators': 634, 'max_depth': 3, 'learning_rate': 0.0012295462168179724, 'subsample': 0.8245472479866144, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:26,737] Trial 161 finished with value: 0.26212744904489305 and parameters: {'n_estimators': 122, 'max_depth': 3, 'learning_rate': 0.040271550637019544, 'subsample': 0.843795682640259, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:26,923] Trial 162 finished with value: 0.25028777535497465 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0013574446236725045, 'subsample': 0.8534538182443405, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:27,112] Trial 163 finished with value: 0.25033993569854485 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.001268718625053911, 'subsample': 0.8739086186595185, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:27,331] Trial 164 finished with value: 0.25027698984498203 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.0018462585487487675, 'subsample': 0.8518304755372493, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:27,698] Trial 165 finished with value: 0.25034631245714445 and parameters: {'n_estimators': 143, 'max_depth': 3, 'learning_rate': 0.0019230770051945965, 'subsample': 0.8528936065636058, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:28,007] Trial 166 finished with value: 0.25040747892729975 and parameters: {'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.0016826956928947638, 'subsample': 0.8617173255024657, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:28,211] Trial 167 finished with value: 0.296345908255217 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.25418026548058775, 'subsample': 0.8501545503320617, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:28,557] Trial 168 finished with value: 0.25054796570572846 and parameters: {'n_estimators': 133, 'max_depth': 4, 'learning_rate': 0.0014936190561752407, 'subsample': 0.8140112455149847, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:28,886] Trial 169 finished with value: 0.25058246826874764 and parameters: {'n_estimators': 181, 'max_depth': 3, 'learning_rate': 0.0021418375375643864, 'subsample': 0.8316949815656209, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:29,674] Trial 170 finished with value: 0.25092867038989575 and parameters: {'n_estimators': 432, 'max_depth': 3, 'learning_rate': 0.0013258110787298565, 'subsample': 0.8419788696340775, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:29,896] Trial 171 finished with value: 0.2503269789813599 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.0011557721875567347, 'subsample': 0.8628261368867567, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:31,029] Trial 172 finished with value: 0.2508681835235748 and parameters: {'n_estimators': 130, 'max_depth': 10, 'learning_rate': 0.0011211111290424197, 'subsample': 0.8505429936626074, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:31,234] Trial 173 finished with value: 0.25031224538863583 and parameters: {'n_estimators': 113, 'max_depth': 3, 'learning_rate': 0.0014072927863275754, 'subsample': 0.8388553643072679, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:31,437] Trial 174 finished with value: 0.2503128129516168 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0011041201619271044, 'subsample': 0.8898101551978986, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:31,679] Trial 175 finished with value: 0.2503178664931596 and parameters: {'n_estimators': 149, 'max_depth': 3, 'learning_rate': 0.0010143835383514652, 'subsample': 0.975090993367439, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:31,871] Trial 176 finished with value: 0.2503355702402905 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.0017859503148892498, 'subsample': 0.8265249865117275, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:32,113] Trial 177 finished with value: 0.25039952510607943 and parameters: {'n_estimators': 142, 'max_depth': 3, 'learning_rate': 0.001576822132906618, 'subsample': 0.8768140885510579, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:32,381] Trial 178 finished with value: 0.25037787403464706 and parameters: {'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.0012793451858067867, 'subsample': 0.9099910119088019, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:32,599] Trial 179 finished with value: 0.250281970029896 and parameters: {'n_estimators': 124, 'max_depth': 3, 'learning_rate': 0.0010114774950228933, 'subsample': 0.8554267639136585, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:32,828] Trial 180 finished with value: 0.25029686302763066 and parameters: {'n_estimators': 134, 'max_depth': 3, 'learning_rate': 0.0014261827101142706, 'subsample': 0.8564681207479546, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:33,055] Trial 181 finished with value: 0.25030428341158767 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.001432455703098761, 'subsample': 0.8564358131652245, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:33,282] Trial 182 finished with value: 0.250319102494431 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.0013023122146097225, 'subsample': 0.8686154661764375, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:33,548] Trial 183 finished with value: 0.2503402473981861 and parameters: {'n_estimators': 159, 'max_depth': 3, 'learning_rate': 0.001226746598093988, 'subsample': 0.8463061330491016, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:33,732] Trial 184 finished with value: 0.25035417486977807 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0016506788470291298, 'subsample': 0.9846027348171561, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:33,937] Trial 185 finished with value: 0.2503013292376782 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.0010020248076437997, 'subsample': 0.8605882888915013, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:34,228] Trial 186 finished with value: 0.25032232719627423 and parameters: {'n_estimators': 142, 'max_depth': 3, 'learning_rate': 0.001450725793576382, 'subsample': 0.836454228814466, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:34,599] Trial 187 finished with value: 0.2504078080156466 and parameters: {'n_estimators': 177, 'max_depth': 3, 'learning_rate': 0.001858044863563851, 'subsample': 0.8525661269628476, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:34,843] Trial 188 finished with value: 0.2503779912944645 and parameters: {'n_estimators': 129, 'max_depth': 3, 'learning_rate': 0.0010075978003716, 'subsample': 0.7473801116225245, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:35,119] Trial 189 finished with value: 0.2503335416552104 and parameters: {'n_estimators': 153, 'max_depth': 3, 'learning_rate': 0.001200869124311189, 'subsample': 0.8655345748638459, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:35,825] Trial 190 finished with value: 0.2506572013643513 and parameters: {'n_estimators': 114, 'max_depth': 7, 'learning_rate': 0.001428272881921993, 'subsample': 0.8442221925933829, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:36,093] Trial 191 finished with value: 0.25026273285535366 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.0011435356054922958, 'subsample': 0.8538943708267991, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:36,291] Trial 192 finished with value: 0.25028273595142375 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0011647024148203242, 'subsample': 0.8541439624369583, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:36,534] Trial 193 finished with value: 0.2503161199241591 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.001155676376671681, 'subsample': 0.8618039196574444, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:36,777] Trial 194 finished with value: 0.25029619113823187 and parameters: {'n_estimators': 113, 'max_depth': 3, 'learning_rate': 0.001003129356138254, 'subsample': 0.8527416902388386, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:37,073] Trial 195 finished with value: 0.2502635536371345 and parameters: {'n_estimators': 140, 'max_depth': 3, 'learning_rate': 0.0010004641901387697, 'subsample': 0.8539427512816576, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:37,346] Trial 196 finished with value: 0.25026989098396835 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.0011635119946692043, 'subsample': 0.8546774579605555, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:37,537] Trial 197 finished with value: 0.25029407976604007 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.00100120402846814, 'subsample': 0.854221551399399, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:37,737] Trial 198 finished with value: 0.25031855236697725 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.001117835637991397, 'subsample': 0.8570320509407681, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:39,038] Trial 199 finished with value: 0.25146026253360226 and parameters: {'n_estimators': 578, 'max_depth': 4, 'learning_rate': 0.001027041062889843, 'subsample': 0.8720163481992307, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:39,359] Trial 200 finished with value: 0.2503108392787646 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.001000599480098359, 'subsample': 0.8529534937212769, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:39,597] Trial 201 finished with value: 0.25032596562009546 and parameters: {'n_estimators': 124, 'max_depth': 3, 'learning_rate': 0.0011929778856590945, 'subsample': 0.8671254046272378, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:39,819] Trial 202 finished with value: 0.25028636160323076 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0012426827820540471, 'subsample': 0.8564398721346151, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:40,069] Trial 203 finished with value: 0.250305276624279 and parameters: {'n_estimators': 139, 'max_depth': 3, 'learning_rate': 0.001305437783122524, 'subsample': 0.8567637923266378, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:40,257] Trial 204 finished with value: 0.2502771657727087 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.0012044948785534797, 'subsample': 0.8408746318842293, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:40,459] Trial 205 finished with value: 0.25029975233779933 and parameters: {'n_estimators': 102, 'max_depth': 3, 'learning_rate': 0.0011569989538935707, 'subsample': 0.842376763070058, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:41,296] Trial 206 finished with value: 0.2506677868334896 and parameters: {'n_estimators': 115, 'max_depth': 9, 'learning_rate': 0.0010089733556623195, 'subsample': 0.8482889808976811, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:41,515] Trial 207 finished with value: 0.25135471732307735 and parameters: {'n_estimators': 124, 'max_depth': 3, 'learning_rate': 0.00691238148418656, 'subsample': 0.8332172943343855, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:41,704] Trial 208 finished with value: 0.2503225549444614 and parameters: {'n_estimators': 102, 'max_depth': 3, 'learning_rate': 0.001233542526982002, 'subsample': 0.8631820013046434, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:41,955] Trial 209 finished with value: 0.2503150256918164 and parameters: {'n_estimators': 148, 'max_depth': 3, 'learning_rate': 0.0012530747961319757, 'subsample': 0.8730783093623724, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:42,146] Trial 210 finished with value: 0.2502986329512068 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0011337441962453714, 'subsample': 0.8420906476472863, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:42,399] Trial 211 finished with value: 0.2502949682426829 and parameters: {'n_estimators': 133, 'max_depth': 3, 'learning_rate': 0.0013512838666597873, 'subsample': 0.8558236044205563, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:42,621] Trial 212 finished with value: 0.2502942717560491 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.0013692941021827103, 'subsample': 0.8528263747127772, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:43,944] Trial 213 finished with value: 0.25206124575152095 and parameters: {'n_estimators': 812, 'max_depth': 3, 'learning_rate': 0.0013609597225575766, 'subsample': 0.8604738912840298, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:44,219] Trial 214 finished with value: 0.2503125588134919 and parameters: {'n_estimators': 137, 'max_depth': 3, 'learning_rate': 0.0015235069884044657, 'subsample': 0.8479845666994155, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:44,497] Trial 215 finished with value: 0.25036884074246213 and parameters: {'n_estimators': 158, 'max_depth': 3, 'learning_rate': 0.0013062065077053835, 'subsample': 0.8676723002733553, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:44,738] Trial 216 finished with value: 0.25032958332049066 and parameters: {'n_estimators': 126, 'max_depth': 3, 'learning_rate': 0.0011545591119927479, 'subsample': 0.8805558230494335, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:44,968] Trial 217 finished with value: 0.25031927857241193 and parameters: {'n_estimators': 120, 'max_depth': 3, 'learning_rate': 0.0015045641047888505, 'subsample': 0.8390473361189064, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:45,249] Trial 218 finished with value: 0.2502882040738902 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.0012772419598724707, 'subsample': 0.8531050489354121, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:45,527] Trial 219 finished with value: 0.2503576268957197 and parameters: {'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.0013088320992278384, 'subsample': 0.8285538828795802, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:45,847] Trial 220 finished with value: 0.2504208251060473 and parameters: {'n_estimators': 174, 'max_depth': 3, 'learning_rate': 0.0016946202287131445, 'subsample': 0.8496216533147206, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:46,108] Trial 221 finished with value: 0.2502883194487484 and parameters: {'n_estimators': 137, 'max_depth': 3, 'learning_rate': 0.0012070609493855492, 'subsample': 0.8555811002009721, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:46,365] Trial 222 finished with value: 0.2502696819842745 and parameters: {'n_estimators': 139, 'max_depth': 3, 'learning_rate': 0.0011220141221442784, 'subsample': 0.8538065006811153, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:46,632] Trial 223 finished with value: 0.250278610414658 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.001151999093595802, 'subsample': 0.8413077242303411, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:46,951] Trial 224 finished with value: 0.25028450562897736 and parameters: {'n_estimators': 152, 'max_depth': 3, 'learning_rate': 0.0011459796381921884, 'subsample': 0.8407910575904263, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:47,271] Trial 225 finished with value: 0.250319151373141 and parameters: {'n_estimators': 166, 'max_depth': 3, 'learning_rate': 0.0011648977309568229, 'subsample': 0.8399193405810258, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:47,616] Trial 226 finished with value: 0.25036404866163536 and parameters: {'n_estimators': 201, 'max_depth': 3, 'learning_rate': 0.0011420295751520956, 'subsample': 0.8327164801281869, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:47,886] Trial 227 finished with value: 0.2502992345461571 and parameters: {'n_estimators': 149, 'max_depth': 3, 'learning_rate': 0.0011581961711858268, 'subsample': 0.8421938342715688, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:48,217] Trial 228 finished with value: 0.25034083739420254 and parameters: {'n_estimators': 186, 'max_depth': 3, 'learning_rate': 0.001003575687354143, 'subsample': 0.8465180068013622, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:48,494] Trial 229 finished with value: 0.2504116409194249 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.0015107698697971888, 'subsample': 0.8214242831478991, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:48,799] Trial 230 finished with value: 0.2503487340171245 and parameters: {'n_estimators': 169, 'max_depth': 3, 'learning_rate': 0.0012834889259143245, 'subsample': 0.8646178486320781, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:49,461] Trial 231 finished with value: 0.2505540255759585 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.001008518849536996, 'subsample': 0.8482917918248972, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:49,733] Trial 232 finished with value: 0.2503274675984707 and parameters: {'n_estimators': 143, 'max_depth': 3, 'learning_rate': 0.001189494874416001, 'subsample': 0.8605078478527121, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:50,079] Trial 233 finished with value: 0.250279333227932 and parameters: {'n_estimators': 129, 'max_depth': 3, 'learning_rate': 0.0011208266052583455, 'subsample': 0.851877039800026, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:50,406] Trial 234 finished with value: 0.25029549344808244 and parameters: {'n_estimators': 133, 'max_depth': 3, 'learning_rate': 0.0013124617407838446, 'subsample': 0.840251358563654, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:50,727] Trial 235 finished with value: 0.25030447758487995 and parameters: {'n_estimators': 156, 'max_depth': 3, 'learning_rate': 0.0011383279806540605, 'subsample': 0.8495375286145671, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:51,001] Trial 236 finished with value: 0.2503411596804725 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.001524147376384906, 'subsample': 0.8331830806104622, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:51,333] Trial 237 finished with value: 0.2503281705709945 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.0011367654707078755, 'subsample': 0.8649951163906425, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:51,675] Trial 238 finished with value: 0.2503099089702564 and parameters: {'n_estimators': 141, 'max_depth': 3, 'learning_rate': 0.0013399411148825278, 'subsample': 0.856737295273591, 'subsample_freq': 5}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:51,979] Trial 239 finished with value: 0.2503411552608922 and parameters: {'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.0012378072307991405, 'subsample': 0.8440669268693588, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:52,162] Trial 240 finished with value: 0.2502804428467461 and parameters: {'n_estimators': 122, 'max_depth': 3, 'learning_rate': 0.001111219162527908, 'subsample': 0.8706717675025365, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:52,320] Trial 241 finished with value: 0.25027977741166163 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.0010963668441895847, 'subsample': 0.8525830680176582, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:52,478] Trial 242 finished with value: 0.25028994212746747 and parameters: {'n_estimators': 113, 'max_depth': 3, 'learning_rate': 0.0011043924797171404, 'subsample': 0.8509721167258081, 'subsample_freq': 6}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:52,651] Trial 243 finished with value: 0.25032392867677455 and parameters: {'n_estimators': 118, 'max_depth': 3, 'learning_rate': 0.0011062362987114417, 'subsample': 0.8628065022838114, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:52,825] Trial 244 finished with value: 0.2502947836125452 and parameters: {'n_estimators': 132, 'max_depth': 3, 'learning_rate': 0.0011285031239596215, 'subsample': 0.8711006911816419, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:52,977] Trial 245 finished with value: 0.25030169184844275 and parameters: {'n_estimators': 121, 'max_depth': 3, 'learning_rate': 0.0012950906667458333, 'subsample': 0.8370169099311032, 'subsample_freq': 5}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:53,154] Trial 246 finished with value: 0.2502972808257372 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.001040615474963279, 'subsample': 0.8456438616187175, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:53,348] Trial 247 finished with value: 0.2502931283300097 and parameters: {'n_estimators': 137, 'max_depth': 3, 'learning_rate': 0.0014776071567393476, 'subsample': 0.85493591309867, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:53,493] Trial 248 finished with value: 0.2503502059871172 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0010020088456990414, 'subsample': 0.7846676123017614, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:53,661] Trial 249 finished with value: 0.2503176043986741 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.0012579292614990783, 'subsample': 0.86165497151801, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:53,870] Trial 250 finished with value: 0.25032871239641014 and parameters: {'n_estimators': 148, 'max_depth': 3, 'learning_rate': 0.001430135847697687, 'subsample': 0.8447472916477398, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:54,213] Trial 251 finished with value: 0.2505005448384193 and parameters: {'n_estimators': 117, 'max_depth': 5, 'learning_rate': 0.0011336494407965842, 'subsample': 0.7988166840369287, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:54,498] Trial 252 finished with value: 0.2553314594765298 and parameters: {'n_estimators': 164, 'max_depth': 3, 'learning_rate': 0.013870845163831977, 'subsample': 0.830954325928814, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:54,666] Trial 253 finished with value: 0.25030351762787606 and parameters: {'n_estimators': 136, 'max_depth': 3, 'learning_rate': 0.0016430332665838402, 'subsample': 0.8513045221369481, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:56,013] Trial 254 finished with value: 0.2522862904164616 and parameters: {'n_estimators': 980, 'max_depth': 3, 'learning_rate': 0.0012343380836218215, 'subsample': 0.8746652144429955, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:56,158] Trial 255 finished with value: 0.250337817402669 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0010059817797579294, 'subsample': 0.9308471396953816, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:56,329] Trial 256 finished with value: 0.2502998221629678 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.0014023490340603418, 'subsample': 0.8393846972364676, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:56,517] Trial 257 finished with value: 0.2503099765058252 and parameters: {'n_estimators': 149, 'max_depth': 3, 'learning_rate': 0.0011564223085492862, 'subsample': 0.8617490828656346, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:56,742] Trial 258 finished with value: 0.2502868178115342 and parameters: {'n_estimators': 129, 'max_depth': 3, 'learning_rate': 0.0010075949533013225, 'subsample': 0.8468369360365582, 'subsample_freq': 1}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:56,893] Trial 259 finished with value: 0.2502906489876938 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.0010015915782827954, 'subsample': 0.8463299432644816, 'subsample_freq': 7}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:57,112] Trial 260 finished with value: 0.25038130553608406 and parameters: {'n_estimators': 176, 'max_depth': 3, 'learning_rate': 0.001138995028056973, 'subsample': 0.8247831006498751, 'subsample_freq': 1}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:57,269] Trial 261 finished with value: 0.2502815376922197 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0010067420029149638, 'subsample': 0.8362422262708763, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:57,443] Trial 262 finished with value: 0.2503346010341051 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.001001305698731606, 'subsample': 0.8379981029899841, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:58,449] Trial 263 finished with value: 0.2520611477195155 and parameters: {'n_estimators': 948, 'max_depth': 3, 'learning_rate': 0.0011176657014634725, 'subsample': 0.8319641802499483, 'subsample_freq': 1}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:58,685] Trial 264 finished with value: 0.25030637886846857 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.001007237686274598, 'subsample': 0.8442513155779106, 'subsample_freq': 1}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:58,874] Trial 265 finished with value: 0.250337217857523 and parameters: {'n_estimators': 125, 'max_depth': 3, 'learning_rate': 0.0013698691384456502, 'subsample': 0.866928965265621, 'subsample_freq': 1}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:59,023] Trial 266 finished with value: 0.25030445216065333 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.001242243478903848, 'subsample': 0.8370416278079784, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:59,158] Trial 267 finished with value: 0.2503014191153362 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.0015596556081260103, 'subsample': 0.8491678208735463, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:59,338] Trial 268 finished with value: 0.43332614817107284 and parameters: {'n_estimators': 161, 'max_depth': 3, 'learning_rate': 0.7478011238659804, 'subsample': 0.8188142703796827, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:59,472] Trial 269 finished with value: 0.2503212378212651 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0010002340080262153, 'subsample': 0.8570535423940846, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:59,650] Trial 270 finished with value: 0.26945947962945394 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.059732721301532336, 'subsample': 0.8270486319425226, 'subsample_freq': 1}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:25:59,894] Trial 271 finished with value: 0.25030863811392523 and parameters: {'n_estimators': 148, 'max_depth': 3, 'learning_rate': 0.0011927364192779342, 'subsample': 0.8425620227324605, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:00,207] Trial 272 finished with value: 0.2814860716819257 and parameters: {'n_estimators': 116, 'max_depth': 4, 'learning_rate': 0.0939743574365124, 'subsample': 0.8696044651004521, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:00,412] Trial 273 finished with value: 0.2503062273781488 and parameters: {'n_estimators': 133, 'max_depth': 3, 'learning_rate': 0.001412955822179249, 'subsample': 0.8493140743856828, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:00,560] Trial 274 finished with value: 0.2502953454753379 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.0011337551123768478, 'subsample': 0.8591189152271943, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:00,938] Trial 275 finished with value: 0.2506860951020985 and parameters: {'n_estimators': 368, 'max_depth': 3, 'learning_rate': 0.0012650188830131861, 'subsample': 0.8359274739980291, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:01,072] Trial 276 finished with value: 0.250744812279008 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.004902098961748642, 'subsample': 0.6934750088062182, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:01,752] Trial 277 finished with value: 0.25213930490953507 and parameters: {'n_estimators': 700, 'max_depth': 3, 'learning_rate': 0.001627296506760647, 'subsample': 0.8556825513571985, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:02,439] Trial 278 finished with value: 0.25062944596570413 and parameters: {'n_estimators': 156, 'max_depth': 7, 'learning_rate': 0.0010048039267808086, 'subsample': 0.8434070349180277, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:02,639] Trial 279 finished with value: 0.25034548537190016 and parameters: {'n_estimators': 135, 'max_depth': 3, 'learning_rate': 0.0013937218189533833, 'subsample': 0.8785742698136241, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:03,254] Trial 280 finished with value: 0.25087093474077005 and parameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.0011340533676453123, 'subsample': 0.8498976409544488, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:03,545] Trial 281 finished with value: 0.2504728057691162 and parameters: {'n_estimators': 172, 'max_depth': 4, 'learning_rate': 0.0012806703801385676, 'subsample': 0.8661169674226017, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:03,686] Trial 282 finished with value: 0.25029289362672735 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.001124380046317621, 'subsample': 0.8302363341515063, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:03,899] Trial 283 finished with value: 0.25046532257023085 and parameters: {'n_estimators': 186, 'max_depth': 3, 'learning_rate': 0.0017519741076071028, 'subsample': 0.86023862291227, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:04,114] Trial 284 finished with value: 0.25038380092615553 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.0014489979598782945, 'subsample': 0.7672583249718715, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:04,269] Trial 285 finished with value: 0.25030014469599016 and parameters: {'n_estimators': 129, 'max_depth': 3, 'learning_rate': 0.00100200885620976, 'subsample': 0.849086294884213, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:04,453] Trial 286 finished with value: 0.250308048240344 and parameters: {'n_estimators': 156, 'max_depth': 3, 'learning_rate': 0.0012432315567774738, 'subsample': 0.8403481592977592, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:04,602] Trial 287 finished with value: 0.25026938855267644 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0011429849844759666, 'subsample': 0.85482750510759, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:04,771] Trial 288 finished with value: 0.250328429733141 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.0011215668300064978, 'subsample': 0.8724761829060815, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:05,421] Trial 289 finished with value: 0.25089230819089825 and parameters: {'n_estimators': 318, 'max_depth': 5, 'learning_rate': 0.0010044151233462926, 'subsample': 0.863388701780957, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:05,894] Trial 290 finished with value: 0.25060752269730413 and parameters: {'n_estimators': 140, 'max_depth': 6, 'learning_rate': 0.0011583099525385595, 'subsample': 0.853392541964113, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:06,067] Trial 291 finished with value: 0.25030998979941127 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.0013247864329417234, 'subsample': 0.9935110620123595, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:06,260] Trial 292 finished with value: 0.25039695254669875 and parameters: {'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.001136738414627039, 'subsample': 0.804988351051009, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:06,420] Trial 293 finished with value: 0.2502891906164376 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.001548558588503069, 'subsample': 0.8371667897982227, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:06,599] Trial 294 finished with value: 0.2503226066947034 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.0012872790527267144, 'subsample': 0.8435880270877928, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:06,788] Trial 295 finished with value: 0.25029803283125324 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.0010023371022914783, 'subsample': 0.8593606013083231, 'subsample_freq': 5}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:06,990] Trial 296 finished with value: 0.25469302720344134 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.022440915122896955, 'subsample': 0.8509800433421202, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:07,178] Trial 297 finished with value: 0.3302497661278953 and parameters: {'n_estimators': 172, 'max_depth': 3, 'learning_rate': 0.3282782386048132, 'subsample': 0.8271363636427567, 'subsample_freq': 7}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:07,327] Trial 298 finished with value: 0.2503316094418712 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.001143241278625727, 'subsample': 0.8667134266212912, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:07,518] Trial 299 finished with value: 0.2503125176914519 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.0014523543291105185, 'subsample': 0.8453665473624608, 'subsample_freq': 1}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:07,729] Trial 300 finished with value: 0.25037612746684823 and parameters: {'n_estimators': 190, 'max_depth': 3, 'learning_rate': 0.0012593210698864372, 'subsample': 0.7300244672583579, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:07,953] Trial 301 finished with value: 0.250406104795899 and parameters: {'n_estimators': 131, 'max_depth': 4, 'learning_rate': 0.0011276833043773978, 'subsample': 0.8330006469394966, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:08,155] Trial 302 finished with value: 0.25035492576344315 and parameters: {'n_estimators': 156, 'max_depth': 3, 'learning_rate': 0.0016970735565812664, 'subsample': 0.8578914822162993, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:08,307] Trial 303 finished with value: 0.25028880560295946 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.001392751084540437, 'subsample': 0.8417559920193403, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:08,471] Trial 304 finished with value: 0.25036276461705487 and parameters: {'n_estimators': 136, 'max_depth': 3, 'learning_rate': 0.0010016098927465764, 'subsample': 0.8177367243831959, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:08,596] Trial 305 finished with value: 0.2502984456201016 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.001230430907031758, 'subsample': 0.8516844867614599, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:09,485] Trial 306 finished with value: 0.25099923634579085 and parameters: {'n_estimators': 166, 'max_depth': 9, 'learning_rate': 0.001137141228181282, 'subsample': 0.8706751537242206, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:09,846] Trial 307 finished with value: 0.2506676329106314 and parameters: {'n_estimators': 124, 'max_depth': 6, 'learning_rate': 0.0014996739273513138, 'subsample': 0.8806872677730566, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:10,035] Trial 308 finished with value: 0.250318391377571 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.0012850326444097948, 'subsample': 0.8602832631381544, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:10,172] Trial 309 finished with value: 0.25028696776960224 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0011237850759421258, 'subsample': 0.847902603706188, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:10,322] Trial 310 finished with value: 0.2502980102678889 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0011200495991954705, 'subsample': 0.8455801884084697, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:10,523] Trial 311 finished with value: 0.25030185175149033 and parameters: {'n_estimators': 112, 'max_depth': 3, 'learning_rate': 0.001003062184381571, 'subsample': 0.8356448577051432, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:10,648] Trial 312 finished with value: 0.2502823878327823 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.001362311743470866, 'subsample': 0.853727770435435, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:10,795] Trial 313 finished with value: 0.25039244597651295 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.0018447521653837443, 'subsample': 0.8645394746836196, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:10,970] Trial 314 finished with value: 0.2503008889798175 and parameters: {'n_estimators': 151, 'max_depth': 3, 'learning_rate': 0.0015692172285326751, 'subsample': 0.8551533321639306, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:11,299] Trial 315 finished with value: 0.25043448923449774 and parameters: {'n_estimators': 129, 'max_depth': 4, 'learning_rate': 0.001379053479188, 'subsample': 0.8732982063372877, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:11,548] Trial 316 finished with value: 0.2503756934005383 and parameters: {'n_estimators': 180, 'max_depth': 3, 'learning_rate': 0.0013006180206010301, 'subsample': 0.8602117165061096, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:12,244] Trial 317 finished with value: 0.251060376312807 and parameters: {'n_estimators': 141, 'max_depth': 8, 'learning_rate': 0.001612284968471793, 'subsample': 0.8382147746384475, 'subsample_freq': 5}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:12,389] Trial 318 finished with value: 0.2502563302812225 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0012691074458087536, 'subsample': 0.9996745877872246, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:12,529] Trial 319 finished with value: 0.2502980779154925 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0014511276245033813, 'subsample': 0.9953265601355363, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:12,677] Trial 320 finished with value: 0.2503870157287665 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.00185379287094357, 'subsample': 0.9920359685276602, 'subsample_freq': 6}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:12,812] Trial 321 finished with value: 0.25030502982478453 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.0012244379780320409, 'subsample': 0.983223316176288, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:12,947] Trial 322 finished with value: 0.2503067922670798 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0013813215325539703, 'subsample': 0.9983733549933829, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:13,076] Trial 323 finished with value: 0.25030927585672214 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0016118563926321079, 'subsample': 0.9989854965217922, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:13,274] Trial 324 finished with value: 0.25040087292218405 and parameters: {'n_estimators': 162, 'max_depth': 3, 'learning_rate': 0.0012436536717371014, 'subsample': 0.8904748175941197, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:13,784] Trial 325 finished with value: 0.2520665083852418 and parameters: {'n_estimators': 129, 'max_depth': 7, 'learning_rate': 0.003961197733000988, 'subsample': 0.8545355554010198, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:14,012] Trial 326 finished with value: 0.25033021425020535 and parameters: {'n_estimators': 141, 'max_depth': 3, 'learning_rate': 0.001165718413443914, 'subsample': 0.9780296520092987, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:14,167] Trial 327 finished with value: 0.2503374645585098 and parameters: {'n_estimators': 118, 'max_depth': 3, 'learning_rate': 0.0014073345875683044, 'subsample': 0.8670246696448006, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:14,412] Trial 328 finished with value: 0.2503516891378939 and parameters: {'n_estimators': 151, 'max_depth': 4, 'learning_rate': 0.0010009119218963765, 'subsample': 0.9516715879711846, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:14,958] Trial 329 finished with value: 0.25139379377844434 and parameters: {'n_estimators': 596, 'max_depth': 3, 'learning_rate': 0.0012872597928015883, 'subsample': 0.8266535313092216, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:15,807] Trial 330 finished with value: 0.25063907256853557 and parameters: {'n_estimators': 120, 'max_depth': 10, 'learning_rate': 0.0011427531104128072, 'subsample': 0.9101427966530633, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:16,053] Trial 331 finished with value: 0.25029250782887846 and parameters: {'n_estimators': 138, 'max_depth': 3, 'learning_rate': 0.0015435079914626936, 'subsample': 0.8561855549387788, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:16,329] Trial 332 finished with value: 0.250523284135558 and parameters: {'n_estimators': 101, 'max_depth': 5, 'learning_rate': 0.0012747548414545778, 'subsample': 0.8774856355080294, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:16,524] Trial 333 finished with value: 0.2503578864954771 and parameters: {'n_estimators': 164, 'max_depth': 3, 'learning_rate': 0.0011383966598968824, 'subsample': 0.968530135724165, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:16,670] Trial 334 finished with value: 0.25030271556664463 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.0017651852446728543, 'subsample': 0.840771840531199, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:17,552] Trial 335 finished with value: 0.27009512498621513 and parameters: {'n_estimators': 896, 'max_depth': 3, 'learning_rate': 0.01059848015592517, 'subsample': 0.8613537969378419, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:17,751] Trial 336 finished with value: 0.2503275648347926 and parameters: {'n_estimators': 193, 'max_depth': 3, 'learning_rate': 0.0010027553929861618, 'subsample': 0.8500257499077014, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:17,909] Trial 337 finished with value: 0.25037331569260834 and parameters: {'n_estimators': 135, 'max_depth': 3, 'learning_rate': 0.0014416392738224569, 'subsample': 0.9918662826691581, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:18,049] Trial 338 finished with value: 0.25041568573243794 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.0011390700601309428, 'subsample': 0.6154438598472899, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:18,232] Trial 339 finished with value: 0.2503277542178327 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0013111755691579486, 'subsample': 0.8681680966608798, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:18,441] Trial 340 finished with value: 0.25035137452381123 and parameters: {'n_estimators': 149, 'max_depth': 3, 'learning_rate': 0.0020396396194361055, 'subsample': 0.855033187815593, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:18,675] Trial 341 finished with value: 0.2503702143381291 and parameters: {'n_estimators': 171, 'max_depth': 3, 'learning_rate': 0.0011294179564234894, 'subsample': 0.9855777281740082, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:18,919] Trial 342 finished with value: 0.26797519930728136 and parameters: {'n_estimators': 130, 'max_depth': 4, 'learning_rate': 0.04162573075498042, 'subsample': 0.8311591556311613, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:19,092] Trial 343 finished with value: 0.2502782410721673 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.001474806841943566, 'subsample': 0.8457126172927842, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:19,285] Trial 344 finished with value: 0.250359896971049 and parameters: {'n_estimators': 147, 'max_depth': 3, 'learning_rate': 0.001695508141906641, 'subsample': 0.8421970356190852, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:19,452] Trial 345 finished with value: 0.25041426439062225 and parameters: {'n_estimators': 128, 'max_depth': 3, 'learning_rate': 0.0014348196961432409, 'subsample': 0.8135462280255692, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:19,618] Trial 346 finished with value: 0.2502989148746415 and parameters: {'n_estimators': 113, 'max_depth': 3, 'learning_rate': 0.0015657374701219828, 'subsample': 0.8359982523365783, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:19,824] Trial 347 finished with value: 0.25029909829831826 and parameters: {'n_estimators': 162, 'max_depth': 3, 'learning_rate': 0.0010003011132425708, 'subsample': 0.8470561292756439, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:20,017] Trial 348 finished with value: 0.2502849574080304 and parameters: {'n_estimators': 136, 'max_depth': 3, 'learning_rate': 0.0012928191744697032, 'subsample': 0.849845907806873, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:20,237] Trial 349 finished with value: 0.25030885065479175 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.0013392116478017524, 'subsample': 0.8417983440809953, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:20,441] Trial 350 finished with value: 0.2503645324694216 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.0016770922950604828, 'subsample': 0.823105637844882, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:20,661] Trial 351 finished with value: 0.2503872222184399 and parameters: {'n_estimators': 171, 'max_depth': 3, 'learning_rate': 0.0012283738250609794, 'subsample': 0.9220428685854014, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:20,878] Trial 352 finished with value: 0.250336728407451 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0014328822423258084, 'subsample': 0.8641122317635307, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:21,081] Trial 353 finished with value: 0.2504457082669491 and parameters: {'n_estimators': 151, 'max_depth': 3, 'learning_rate': 0.0018841034365622655, 'subsample': 0.9996367425929288, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:21,317] Trial 354 finished with value: 0.26553881794480605 and parameters: {'n_estimators': 210, 'max_depth': 3, 'learning_rate': 0.030703605306946786, 'subsample': 0.850111121648392, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:21,486] Trial 355 finished with value: 0.25030200002886194 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.0011079167752437269, 'subsample': 0.8337317453203524, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:21,704] Trial 356 finished with value: 0.25035797502933815 and parameters: {'n_estimators': 183, 'max_depth': 3, 'learning_rate': 0.0013309552007959905, 'subsample': 0.8557263641544529, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:21,918] Trial 357 finished with value: 0.29560287488792175 and parameters: {'n_estimators': 128, 'max_depth': 4, 'learning_rate': 0.13984050186977195, 'subsample': 0.8731708070235611, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:22,100] Trial 358 finished with value: 0.2503046692564398 and parameters: {'n_estimators': 143, 'max_depth': 3, 'learning_rate': 0.001134881438639518, 'subsample': 0.8465608685528766, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:22,239] Trial 359 finished with value: 0.250304309091803 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0010003567465016502, 'subsample': 0.8603234779917641, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:22,410] Trial 360 finished with value: 0.25046942804848876 and parameters: {'n_estimators': 133, 'max_depth': 3, 'learning_rate': 0.001459238392902932, 'subsample': 0.6605875187490169, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:22,564] Trial 361 finished with value: 0.25027536955830504 and parameters: {'n_estimators': 113, 'max_depth': 3, 'learning_rate': 0.0012482062469152673, 'subsample': 0.8400036352226288, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:22,745] Trial 362 finished with value: 0.25034265642162856 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.001130706095751181, 'subsample': 0.7942445475493485, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:22,877] Trial 363 finished with value: 0.25032819984357324 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.001630307564547665, 'subsample': 0.8285532231844901, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:23,170] Trial 364 finished with value: 0.2511475867729362 and parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.002379049928317518, 'subsample': 0.8368208257876575, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:23,370] Trial 365 finished with value: 0.25034839375230816 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.0012439005362270882, 'subsample': 0.8210359139720524, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:24,709] Trial 366 finished with value: 0.2525638423284782 and parameters: {'n_estimators': 667, 'max_depth': 5, 'learning_rate': 0.0011368282730408671, 'subsample': 0.8415195589178222, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:24,895] Trial 367 finished with value: 0.25038455766955925 and parameters: {'n_estimators': 117, 'max_depth': 4, 'learning_rate': 0.001476444273792373, 'subsample': 0.8635289327057959, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:25,079] Trial 368 finished with value: 0.25028805280598093 and parameters: {'n_estimators': 155, 'max_depth': 3, 'learning_rate': 0.0010026377824965721, 'subsample': 0.85260592644126, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:25,491] Trial 369 finished with value: 0.2509901947030741 and parameters: {'n_estimators': 445, 'max_depth': 3, 'learning_rate': 0.0012693173335051333, 'subsample': 0.750062889243568, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:25,617] Trial 370 finished with value: 0.25029928236822563 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0010040321866108808, 'subsample': 0.8363603926010676, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:25,773] Trial 371 finished with value: 0.25036731991241046 and parameters: {'n_estimators': 136, 'max_depth': 3, 'learning_rate': 0.0020670260801591844, 'subsample': 0.8441161609524632, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:25,942] Trial 372 finished with value: 0.25039994326940457 and parameters: {'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.001288697648270419, 'subsample': 0.9899039573913935, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:26,082] Trial 373 finished with value: 0.25031734940334427 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.0011432866107321275, 'subsample': 0.8973852701620973, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:26,246] Trial 374 finished with value: 0.25027752427200944 and parameters: {'n_estimators': 127, 'max_depth': 3, 'learning_rate': 0.0015185646614750917, 'subsample': 0.8551245308676, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:26,396] Trial 375 finished with value: 0.25039473599245043 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0018640834805959587, 'subsample': 0.8801456883886075, 'subsample_freq': 7}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:26,553] Trial 376 finished with value: 0.25036427193320077 and parameters: {'n_estimators': 126, 'max_depth': 3, 'learning_rate': 0.0016223576616168796, 'subsample': 0.866986961695829, 'subsample_freq': 6}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:26,767] Trial 377 finished with value: 0.2503134691559824 and parameters: {'n_estimators': 139, 'max_depth': 3, 'learning_rate': 0.001481807820773126, 'subsample': 0.8579986719092322, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:26,909] Trial 378 finished with value: 0.2502848436889902 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.001752573242883089, 'subsample': 0.8531214328507357, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:27,312] Trial 379 finished with value: 0.2508251570446647 and parameters: {'n_estimators': 387, 'max_depth': 3, 'learning_rate': 0.0014169835857788847, 'subsample': 0.8710373312404532, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:27,543] Trial 380 finished with value: 0.2503094296943814 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.001276896974555224, 'subsample': 0.8608738639185408, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:27,831] Trial 381 finished with value: 0.2503095478262165 and parameters: {'n_estimators': 133, 'max_depth': 3, 'learning_rate': 0.001544350378651009, 'subsample': 0.8511225358096223, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:28,433] Trial 382 finished with value: 0.2503125357298927 and parameters: {'n_estimators': 117, 'max_depth': 4, 'learning_rate': 0.0010001397957469288, 'subsample': 0.8608082745194976, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:28,827] Trial 383 finished with value: 0.2502897485136518 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.0012650517845322427, 'subsample': 0.8485545416218937, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:34,767] Trial 384 finished with value: 0.25549856655131187 and parameters: {'n_estimators': 782, 'max_depth': 8, 'learning_rate': 0.00114089172274259, 'subsample': 0.7802375169739986, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:34,980] Trial 385 finished with value: 0.2503425440198049 and parameters: {'n_estimators': 147, 'max_depth': 3, 'learning_rate': 0.001468223521066897, 'subsample': 0.8695407060675415, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:35,142] Trial 386 finished with value: 0.2504052380945302 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.0020720262789321827, 'subsample': 0.8293654172938842, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:35,343] Trial 387 finished with value: 0.2503689462523919 and parameters: {'n_estimators': 173, 'max_depth': 3, 'learning_rate': 0.001131949080736695, 'subsample': 0.8846506399563155, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:35,498] Trial 388 finished with value: 0.25032103239765846 and parameters: {'n_estimators': 119, 'max_depth': 3, 'learning_rate': 0.001731298415440471, 'subsample': 0.842907667424187, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:36,264] Trial 389 finished with value: 0.25052610085148574 and parameters: {'n_estimators': 155, 'max_depth': 7, 'learning_rate': 0.0013188930423912106, 'subsample': 0.9751013926139916, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:36,527] Trial 390 finished with value: 0.250279961284076 and parameters: {'n_estimators': 136, 'max_depth': 3, 'learning_rate': 0.0011268919466271461, 'subsample': 0.8552330509121336, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:36,865] Trial 391 finished with value: 0.2503852516031603 and parameters: {'n_estimators': 190, 'max_depth': 3, 'learning_rate': 0.0011094329404271795, 'subsample': 0.8762572269732296, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:37,052] Trial 392 finished with value: 0.2502985263601082 and parameters: {'n_estimators': 140, 'max_depth': 3, 'learning_rate': 0.001005262691327262, 'subsample': 0.8567773244132739, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:37,263] Trial 393 finished with value: 0.25034975811669774 and parameters: {'n_estimators': 168, 'max_depth': 3, 'learning_rate': 0.0012237926913345062, 'subsample': 0.8644504157982076, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:37,538] Trial 394 finished with value: 0.25032974883663844 and parameters: {'n_estimators': 131, 'max_depth': 4, 'learning_rate': 0.001161580324323505, 'subsample': 0.8540429159651645, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:37,987] Trial 395 finished with value: 0.2507276575821492 and parameters: {'n_estimators': 153, 'max_depth': 6, 'learning_rate': 0.0013190046795519004, 'subsample': 0.8478220281188458, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:38,129] Trial 396 finished with value: 0.25031048307004466 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.0011145713078375432, 'subsample': 0.9859119568961507, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:38,325] Trial 397 finished with value: 0.2514357456965723 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.007251243949538284, 'subsample': 0.9451951654336616, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:38,609] Trial 398 finished with value: 0.2503185938211763 and parameters: {'n_estimators': 142, 'max_depth': 3, 'learning_rate': 0.0013772325358045324, 'subsample': 0.8356408329122419, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:38,872] Trial 399 finished with value: 0.25031437401248396 and parameters: {'n_estimators': 128, 'max_depth': 3, 'learning_rate': 0.0010034329318931825, 'subsample': 0.8632765930320245, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:39,254] Trial 400 finished with value: 0.2503500899011681 and parameters: {'n_estimators': 164, 'max_depth': 3, 'learning_rate': 0.0012605097632084356, 'subsample': 0.8422496654832927, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:39,469] Trial 401 finished with value: 0.25027431456854304 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.001000803765179772, 'subsample': 0.8546636042942207, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:39,627] Trial 402 finished with value: 0.2503174614155103 and parameters: {'n_estimators': 140, 'max_depth': 3, 'learning_rate': 0.0010914179290343624, 'subsample': 0.8733266746361353, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:39,773] Trial 403 finished with value: 0.2503137331886058 and parameters: {'n_estimators': 125, 'max_depth': 3, 'learning_rate': 0.00101027910645552, 'subsample': 0.8570763677746506, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:40,043] Trial 404 finished with value: 0.2505623548070123 and parameters: {'n_estimators': 263, 'max_depth': 3, 'learning_rate': 0.0012216739906076462, 'subsample': 0.8082418646845648, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:40,237] Trial 405 finished with value: 0.25037311226593956 and parameters: {'n_estimators': 175, 'max_depth': 3, 'learning_rate': 0.0014239493866888284, 'subsample': 0.8468175812127261, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:40,433] Trial 406 finished with value: 0.2503102000645379 and parameters: {'n_estimators': 155, 'max_depth': 3, 'learning_rate': 0.0011235073958310495, 'subsample': 0.9994972206086031, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:40,578] Trial 407 finished with value: 0.2503069749741898 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.001009282438019388, 'subsample': 0.8621775217992173, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:41,012] Trial 408 finished with value: 0.2536804201214953 and parameters: {'n_estimators': 142, 'max_depth': 6, 'learning_rate': 0.0055332313397114495, 'subsample': 0.8359490906884777, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:41,219] Trial 409 finished with value: 0.25043522017974496 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.0015567901194347142, 'subsample': 0.8516735463226668, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:41,481] Trial 410 finished with value: 0.25038739512138075 and parameters: {'n_estimators': 130, 'max_depth': 4, 'learning_rate': 0.0012829051475726843, 'subsample': 0.8656297456336857, 'subsample_freq': 5}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:41,623] Trial 411 finished with value: 0.25029940848761867 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.001161147434591814, 'subsample': 0.8432538873938159, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:41,821] Trial 412 finished with value: 0.25031597938199923 and parameters: {'n_estimators': 154, 'max_depth': 3, 'learning_rate': 0.0013606930236909783, 'subsample': 0.8563527006174804, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:42,214] Trial 413 finished with value: 0.2505570116892719 and parameters: {'n_estimators': 336, 'max_depth': 3, 'learning_rate': 0.0010042711268572696, 'subsample': 0.9311644018529619, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:42,387] Trial 414 finished with value: 0.25030330959071134 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.001603689376609267, 'subsample': 0.8328386904277649, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:42,581] Trial 415 finished with value: 0.25029342773113605 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.00100059163646106, 'subsample': 0.9642600145677935, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:42,904] Trial 416 finished with value: 0.2504198028554244 and parameters: {'n_estimators': 181, 'max_depth': 3, 'learning_rate': 0.001236035319457025, 'subsample': 0.8213658220477575, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:43,285] Trial 417 finished with value: 0.2505393365009619 and parameters: {'n_estimators': 147, 'max_depth': 5, 'learning_rate': 0.001400294457325977, 'subsample': 0.8691858594365501, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:43,436] Trial 418 finished with value: 0.2502738818461462 and parameters: {'n_estimators': 117, 'max_depth': 3, 'learning_rate': 0.0011607364192804852, 'subsample': 0.8519804361530882, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:43,626] Trial 419 finished with value: 0.2503506958038876 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.0011668033732049464, 'subsample': 0.8449264901719106, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:43,844] Trial 420 finished with value: 0.25029066789392207 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.0011153547246180128, 'subsample': 0.8396340494290087, 'subsample_freq': 6}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:44,134] Trial 421 finished with value: 0.2503319681494525 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.0010011945071304342, 'subsample': 0.7045589131949701, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:44,317] Trial 422 finished with value: 0.2503052591549205 and parameters: {'n_estimators': 118, 'max_depth': 3, 'learning_rate': 0.0012002768807982795, 'subsample': 0.8275116733275922, 'subsample_freq': 3}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:44,670] Trial 423 finished with value: 0.25037637421312076 and parameters: {'n_estimators': 233, 'max_depth': 3, 'learning_rate': 0.0011218856819975582, 'subsample': 0.8496532384498622, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:44,832] Trial 424 finished with value: 0.25031416498133935 and parameters: {'n_estimators': 127, 'max_depth': 3, 'learning_rate': 0.0012711757857987646, 'subsample': 0.860218774774153, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:45,104] Trial 425 finished with value: 0.25042626665939 and parameters: {'n_estimators': 160, 'max_depth': 3, 'learning_rate': 0.0014772225091908256, 'subsample': 0.8813309587541239, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:45,367] Trial 426 finished with value: 0.2502980529153391 and parameters: {'n_estimators': 118, 'max_depth': 3, 'learning_rate': 0.0011302858627055, 'subsample': 0.9912236900270273, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:45,606] Trial 427 finished with value: 0.2502829940011675 and parameters: {'n_estimators': 141, 'max_depth': 3, 'learning_rate': 0.0012697687770504185, 'subsample': 0.8524791746788577, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:45,784] Trial 428 finished with value: 0.25029842626176524 and parameters: {'n_estimators': 115, 'max_depth': 3, 'learning_rate': 0.0015679039412153709, 'subsample': 0.8710056670522419, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:46,078] Trial 429 finished with value: 0.25039805218819017 and parameters: {'n_estimators': 180, 'max_depth': 4, 'learning_rate': 0.0010086962614606594, 'subsample': 0.8400468881041203, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:46,285] Trial 430 finished with value: 0.2503092298258779 and parameters: {'n_estimators': 143, 'max_depth': 3, 'learning_rate': 0.0013882172810694622, 'subsample': 0.8592067725512061, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:47,070] Trial 431 finished with value: 0.25115055933022007 and parameters: {'n_estimators': 127, 'max_depth': 9, 'learning_rate': 0.0017910336606358458, 'subsample': 0.8480846291154781, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:47,268] Trial 432 finished with value: 0.2503084315807663 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.00112973467736928, 'subsample': 0.830219643181733, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:47,460] Trial 433 finished with value: 0.3504639355769323 and parameters: {'n_estimators': 163, 'max_depth': 3, 'learning_rate': 0.45959987044817047, 'subsample': 0.9802948507031303, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:47,609] Trial 434 finished with value: 0.25032331855519485 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.0012440215809205047, 'subsample': 0.8664320193413284, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:47,783] Trial 435 finished with value: 0.2502731709149137 and parameters: {'n_estimators': 141, 'max_depth': 3, 'learning_rate': 0.0011560501660454734, 'subsample': 0.8549111342599988, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:47,964] Trial 436 finished with value: 0.2566556985058583 and parameters: {'n_estimators': 154, 'max_depth': 3, 'learning_rate': 0.01982917238961967, 'subsample': 0.8445482987790369, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:48,189] Trial 437 finished with value: 0.27072446135291844 and parameters: {'n_estimators': 187, 'max_depth': 3, 'learning_rate': 0.04857406930895704, 'subsample': 0.8555010110003188, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:48,362] Trial 438 finished with value: 0.25029582079969936 and parameters: {'n_estimators': 138, 'max_depth': 3, 'learning_rate': 0.001427932599773867, 'subsample': 0.8403721835540912, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:48,572] Trial 439 finished with value: 0.25042258431927766 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.001622441324837556, 'subsample': 0.8638945139362035, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:48,811] Trial 440 finished with value: 0.2504300531066918 and parameters: {'n_estimators': 141, 'max_depth': 4, 'learning_rate': 0.0012524942713061076, 'subsample': 0.8762747052920742, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:49,293] Trial 441 finished with value: 0.2508675925385875 and parameters: {'n_estimators': 491, 'max_depth': 3, 'learning_rate': 0.0011450780527326892, 'subsample': 0.8501421441252427, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:49,520] Trial 442 finished with value: 0.25030225492743746 and parameters: {'n_estimators': 114, 'max_depth': 3, 'learning_rate': 0.0013848204987172974, 'subsample': 0.8346540262484287, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:49,704] Trial 443 finished with value: 0.25300752584967445 and parameters: {'n_estimators': 151, 'max_depth': 3, 'learning_rate': 0.00932223869898192, 'subsample': 0.8583667925715117, 'subsample_freq': 5}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:49,874] Trial 444 finished with value: 0.2504215663980064 and parameters: {'n_estimators': 128, 'max_depth': 3, 'learning_rate': 0.0018569579113571753, 'subsample': 0.9045937194306566, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:50,037] Trial 445 finished with value: 0.25036378501526313 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.0010036666625255633, 'subsample': 0.8129313665219067, 'subsample_freq': 7}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:50,538] Trial 446 finished with value: 0.25058872738347493 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.0011625890025329269, 'subsample': 0.8468146563546256, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:50,903] Trial 447 finished with value: 0.250573138128373 and parameters: {'n_estimators': 115, 'max_depth': 6, 'learning_rate': 0.0013292239605970076, 'subsample': 0.8258856822234721, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:51,098] Trial 448 finished with value: 0.25032201680317573 and parameters: {'n_estimators': 161, 'max_depth': 3, 'learning_rate': 0.0015329030115393495, 'subsample': 0.8543529078577281, 'subsample_freq': 2}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:51,320] Trial 449 finished with value: 0.2503488711776055 and parameters: {'n_estimators': 197, 'max_depth': 3, 'learning_rate': 0.0011320098593283587, 'subsample': 0.84198239895987, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:51,508] Trial 450 finished with value: 0.2503504650709349 and parameters: {'n_estimators': 139, 'max_depth': 3, 'learning_rate': 0.0013513446272475926, 'subsample': 0.8681659989138174, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:52,067] Trial 451 finished with value: 0.25109942563868587 and parameters: {'n_estimators': 561, 'max_depth': 3, 'learning_rate': 0.0011577372603265546, 'subsample': 0.8868108240363478, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:52,305] Trial 452 finished with value: 0.25023718907647274 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.0016913105802529214, 'subsample': 0.9902023757498153, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:52,558] Trial 453 finished with value: 0.2509667652010056 and parameters: {'n_estimators': 101, 'max_depth': 5, 'learning_rate': 0.002705437335017345, 'subsample': 0.9616827784160639, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:52,766] Trial 454 finished with value: 0.2504156339848843 and parameters: {'n_estimators': 118, 'max_depth': 4, 'learning_rate': 0.0021730174128242884, 'subsample': 0.9861821359611156, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:53,032] Trial 455 finished with value: 0.2505798209060284 and parameters: {'n_estimators': 174, 'max_depth': 4, 'learning_rate': 0.0016842155288074145, 'subsample': 0.9882285113629437, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:53,255] Trial 456 finished with value: 0.2504453491782903 and parameters: {'n_estimators': 136, 'max_depth': 4, 'learning_rate': 0.0018664953211911578, 'subsample': 0.9925317965201671, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:53,527] Trial 457 finished with value: 0.25040029512622514 and parameters: {'n_estimators': 114, 'max_depth': 5, 'learning_rate': 0.0016710182657310765, 'subsample': 0.9842486158576028, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:53,765] Trial 458 finished with value: 0.25058027431095087 and parameters: {'n_estimators': 152, 'max_depth': 4, 'learning_rate': 0.0019940691466023186, 'subsample': 0.9799782387679259, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:53,939] Trial 459 finished with value: 0.2545505913931825 and parameters: {'n_estimators': 129, 'max_depth': 3, 'learning_rate': 0.014443522745879353, 'subsample': 0.9709134936016748, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:54,321] Trial 460 finished with value: 0.2503465962841329 and parameters: {'n_estimators': 113, 'max_depth': 5, 'learning_rate': 0.0014797144781833616, 'subsample': 0.9926459929447164, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:54,848] Trial 461 finished with value: 0.25290775896383133 and parameters: {'n_estimators': 425, 'max_depth': 3, 'learning_rate': 0.00325901196586806, 'subsample': 0.8609261520377366, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:55,094] Trial 462 finished with value: 0.2502151467831446 and parameters: {'n_estimators': 101, 'max_depth': 5, 'learning_rate': 0.0015873361357196063, 'subsample': 0.9993488737514519, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:55,338] Trial 463 finished with value: 0.2503939993096339 and parameters: {'n_estimators': 104, 'max_depth': 5, 'learning_rate': 0.002014865693984426, 'subsample': 0.9955227018041929, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:55,729] Trial 464 finished with value: 0.3094356015866288 and parameters: {'n_estimators': 101, 'max_depth': 6, 'learning_rate': 0.21065455446961356, 'subsample': 0.9791220538626532, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:55,988] Trial 465 finished with value: 0.2502699141997187 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.0017433722663255078, 'subsample': 0.9963836245485508, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:57,758] Trial 466 finished with value: 0.257902199604487 and parameters: {'n_estimators': 726, 'max_depth': 5, 'learning_rate': 0.00211419341082173, 'subsample': 0.9942843211238956, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:58,151] Trial 467 finished with value: 0.2510604148351244 and parameters: {'n_estimators': 114, 'max_depth': 6, 'learning_rate': 0.002328861528050288, 'subsample': 0.9892207371220401, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:58,401] Trial 468 finished with value: 0.2506530380739005 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.0024122932489312418, 'subsample': 0.9878569315403097, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:58,648] Trial 469 finished with value: 0.25038189684340206 and parameters: {'n_estimators': 101, 'max_depth': 5, 'learning_rate': 0.0017648767841674452, 'subsample': 0.9885706802337865, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:58,939] Trial 470 finished with value: 0.2503699048459322 and parameters: {'n_estimators': 128, 'max_depth': 5, 'learning_rate': 0.0016444548490556876, 'subsample': 0.998662603937842, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:59,383] Trial 471 finished with value: 0.2505787074108867 and parameters: {'n_estimators': 126, 'max_depth': 6, 'learning_rate': 0.001792460900526482, 'subsample': 0.9726085127347254, 'subsample_freq': 8}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:26:59,566] Trial 472 finished with value: 0.2503068986017657 and parameters: {'n_estimators': 101, 'max_depth': 4, 'learning_rate': 0.0015501151075108276, 'subsample': 0.9683872182432194, 'subsample_freq': 4}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:27:01,922] Trial 473 finished with value: 0.2581037603748745 and parameters: {'n_estimators': 864, 'max_depth': 5, 'learning_rate': 0.0018837952140982254, 'subsample': 0.9804514000275032, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:27:02,581] Trial 474 finished with value: 0.25051111196040005 and parameters: {'n_estimators': 127, 'max_depth': 6, 'learning_rate': 0.0015463790482869778, 'subsample': 0.9848830593400936, 'subsample_freq': 9}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:27:02,972] Trial 475 finished with value: 0.278105070582492 and parameters: {'n_estimators': 141, 'max_depth': 4, 'learning_rate': 0.06880512244503745, 'subsample': 0.9988263610570626, 'subsample_freq': 10}. Best is trial 78 with value: 0.250184913694273.\n",
      "[I 2024-11-16 13:27:03,603] Trial 476 finished with value: 0.2500921569622224 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.0014706591664152204, 'subsample': 0.9977530576958129, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:04,302] Trial 477 finished with value: 0.25048925393152704 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.0020099055055494905, 'subsample': 0.9922191426373294, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:04,909] Trial 478 finished with value: 0.25054003599308794 and parameters: {'n_estimators': 114, 'max_depth': 6, 'learning_rate': 0.0017753999979624887, 'subsample': 0.9766160772513826, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:05,658] Trial 479 finished with value: 0.25131336336779675 and parameters: {'n_estimators': 120, 'max_depth': 7, 'learning_rate': 0.0026275895581321174, 'subsample': 0.9924421748301961, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:06,360] Trial 480 finished with value: 0.2501476328678419 and parameters: {'n_estimators': 101, 'max_depth': 7, 'learning_rate': 0.0015499894697328985, 'subsample': 0.9988055956346313, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:07,123] Trial 481 finished with value: 0.25072548459056565 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.002196848018303854, 'subsample': 0.9994374001528303, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:07,869] Trial 482 finished with value: 0.25031483022291495 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.0017422297651608991, 'subsample': 0.9952632626339625, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:08,658] Trial 483 finished with value: 0.25038771762509177 and parameters: {'n_estimators': 118, 'max_depth': 7, 'learning_rate': 0.0015650624378114408, 'subsample': 0.9994899320405447, 'subsample_freq': 9}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:09,433] Trial 484 finished with value: 0.25034184956393135 and parameters: {'n_estimators': 115, 'max_depth': 7, 'learning_rate': 0.001506350597874688, 'subsample': 0.9837965820650392, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:10,394] Trial 485 finished with value: 0.25052896456378154 and parameters: {'n_estimators': 117, 'max_depth': 7, 'learning_rate': 0.0017944102122863254, 'subsample': 0.9983564931801847, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:11,170] Trial 486 finished with value: 0.2501230526691023 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.001453138794325112, 'subsample': 0.9996574003900875, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:11,751] Trial 487 finished with value: 0.2503509653242976 and parameters: {'n_estimators': 101, 'max_depth': 6, 'learning_rate': 0.0016298562827977653, 'subsample': 0.9991671063206471, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:12,694] Trial 488 finished with value: 0.2506348532512266 and parameters: {'n_estimators': 116, 'max_depth': 7, 'learning_rate': 0.001954684454849229, 'subsample': 0.9876076690472083, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:16,837] Trial 489 finished with value: 0.2560952827566151 and parameters: {'n_estimators': 606, 'max_depth': 7, 'learning_rate': 0.0014610991180166587, 'subsample': 0.9834868375805702, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:17,682] Trial 490 finished with value: 0.2505349365300317 and parameters: {'n_estimators': 101, 'max_depth': 7, 'learning_rate': 0.002045363319760078, 'subsample': 0.9916442456769707, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:18,379] Trial 491 finished with value: 0.2502554962171665 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.00164985365064984, 'subsample': 0.9998810515313944, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:19,197] Trial 492 finished with value: 0.25062377118700246 and parameters: {'n_estimators': 116, 'max_depth': 7, 'learning_rate': 0.0017978651051629379, 'subsample': 0.9991936426473041, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:20,068] Trial 493 finished with value: 0.2510788294041819 and parameters: {'n_estimators': 116, 'max_depth': 7, 'learning_rate': 0.0022683031912782356, 'subsample': 0.999565045573135, 'subsample_freq': 9}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:20,819] Trial 494 finished with value: 0.2503157538045223 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.0016658422922593821, 'subsample': 0.9898461311537695, 'subsample_freq': 8}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:24,512] Trial 495 finished with value: 0.262642516156845 and parameters: {'n_estimators': 541, 'max_depth': 7, 'learning_rate': 0.0024346634588302257, 'subsample': 0.999630969419086, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:25,321] Trial 496 finished with value: 0.2502617106767015 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.0014449431659497877, 'subsample': 0.9811840774576635, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:26,094] Trial 497 finished with value: 0.2507216202759908 and parameters: {'n_estimators': 133, 'max_depth': 6, 'learning_rate': 0.0018263025683725568, 'subsample': 0.9759040725846898, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:26,941] Trial 498 finished with value: 0.2502951020616762 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.0014623188517780682, 'subsample': 0.9774860525031088, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n",
      "[I 2024-11-16 13:27:27,749] Trial 499 finished with value: 0.25027625620837 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.001633255903801869, 'subsample': 0.9842404169311908, 'subsample_freq': 10}. Best is trial 476 with value: 0.2500921569622224.\n"
     ]
    }
   ],
   "source": [
    "def optunaobjective(trial, X, y, cv, preprocessor):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        pred = model.predict(X_val_processed)\n",
    "\n",
    "        score = mean_squared_error(y_val, pred)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "study.optimize(lambda trial: optunaobjective(trial, X, y, cv, preprocessor), n_trials=500)\n",
    "\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_80TzSAosiKc"
   },
   "source": [
    "### 4.3 Tree-based Models (GBR, XGBoost, LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Kc_JTUMWK6xd"
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "gb_best = GradientBoostingRegressor(n_estimators=best_params['n_estimators'],\n",
    "                                    max_depth=best_params['max_depth'],\n",
    "                                    learning_rate=best_params['learning_rate'],\n",
    "                                    subsample=best_params['subsample'],\n",
    "                                    random_state=42)\n",
    "\n",
    "# XGBoost Regressor\n",
    "# Using **best_params to pass all optimized parameters at once\n",
    "xgb_best = XGBRegressor(**best_params, random_state=42)\n",
    "\n",
    "# LightGBM Regressor\n",
    "# using **best_params for consistency with XGBoost\n",
    "lgb_best = LGBMRegressor(**best_params, random_state=42)\n",
    "\n",
    "# Note: These models are initialized with optimized hyperparameters.\n",
    "# The 'best_params' dictionary is assumed to be obtained from a previous optimization process.\n",
    "# All models use random_state=42 for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss_w7xcKsiKd"
   },
   "source": [
    "### 4.4 Linear Models (Elastic Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9YkcA2EUK6t-"
   },
   "outputs": [],
   "source": [
    "# Elastic Net for L1 and L2 regularization\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ClYexHnTK6mK"
   },
   "outputs": [],
   "source": [
    "common_index = X.index.intersection(y.index)\n",
    "X = X.loc[common_index]\n",
    "y = y.loc[common_index]\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "split_date = X.index[train_size]\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:split_date], y[split_date:]\n",
    "\n",
    "train_index = X_train.index\n",
    "test_index = X_test.index\n",
    "\n",
    "original_feature_names = X_train.columns.tolist()\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "if X_train.shape[1] != len(original_feature_names):\n",
    "    processed_feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "else:\n",
    "    processed_feature_names = original_feature_names\n",
    "\n",
    "X_train = pd.DataFrame(X_train, index=train_index, columns=processed_feature_names)\n",
    "X_test = pd.DataFrame(X_test, index=test_index, columns=processed_feature_names)\n",
    "\n",
    "selected_features = processed_feature_names\n",
    "\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_test = y_test.loc[X_test.index]\n",
    "\n",
    "original_index = X_train.index\n",
    "sample_weights = np.linspace(0.5, 1, len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AP36CxfdK6jc",
    "outputId": "19d6c1f3-c663-4c25-9d31-90f8aec732f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1539, 6)\n",
      "y_train shape: (1539,)\n",
      "sample_weights shape: (1539,)\n",
      "NaN in X_train: 0\n",
      "NaN in y_train: 0\n",
      "NaN in sample_weights: 0\n",
      "Length of X_train: 1539\n",
      "Length of y_train: 1539\n",
      "Length of sample_weights: 1539\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X) * 0.8)\n",
    "split_date = X.index[train_size]\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y.loc[:split_date], y.loc[split_date:]\n",
    "\n",
    "X_train_df = X_train.copy()\n",
    "y_train_df = y_train.copy()\n",
    "\n",
    "sample_weights_series = pd.Series(sample_weights, index=X_train.index)\n",
    "\n",
    "aligned_index = X_train_df.index.intersection(y_train_df.index).intersection(sample_weights_series.index)\n",
    "X_train_df = X_train_df.loc[aligned_index]\n",
    "y_train_df = y_train_df.loc[aligned_index]\n",
    "sample_weights_series = sample_weights_series.loc[aligned_index]\n",
    "\n",
    "# Converting back to numpy arrays\n",
    "X_train = X_train_df.to_numpy()\n",
    "y_train = y_train_df.to_numpy()\n",
    "sample_weights = sample_weights_series.to_numpy()\n",
    "\n",
    "# Converting to float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "sample_weights = sample_weights.astype(np.float32)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"sample_weights shape:\", sample_weights.shape)\n",
    "print(\"NaN in X_train:\", np.isnan(X_train).sum())\n",
    "print(\"NaN in y_train:\", np.isnan(y_train).sum())\n",
    "print(\"NaN in sample_weights:\", np.isnan(sample_weights).sum())\n",
    "\n",
    "\n",
    "print(f\"Length of X_train: {len(X_train)}\")\n",
    "print(f\"Length of y_train: {len(y_train)}\")\n",
    "print(f\"Length of sample_weights: {len(sample_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_N38tdwJK6hO",
    "outputId": "1617b5bc-5301-4f38-efe6-3185cf4aebb4"
   },
   "outputs": [],
   "source": [
    "if isinstance(X_train, pd.DataFrame):\n",
    "    X_train = X_train.to_numpy()\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    X_test = X_test.to_numpy()\n",
    "\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Creating and training the LSTM model\n",
    "lstm_model = create_lstm_model((1, X_train.shape[1]), dropout_rate=0.5, neurons=[512, 256, 128])\n",
    "\n",
    "lstm_model.fit(\n",
    "    X_train_lstm,\n",
    "    y_train,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lstm_pred = lstm_model.predict(X_test_lstm).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSNMKeXfsiKd"
   },
   "source": [
    "### 4.5 Time Series Models (SARIMA, Exponential Smoothing, Prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVKhNlw_K6ex"
   },
   "outputs": [],
   "source": [
    "# SARIMA model creation and prediction\n",
    "\n",
    "# Using auto_arima to automatically find the best SARIMA parameters\n",
    "# seasonal=True: Enable seasonal component\n",
    "# m=7: Set the seasonal period to 7\n",
    "auto_arima_model = auto_arima(y_train, seasonal=True, m=7)\n",
    "\n",
    "sarima_order = auto_arima_model.order\n",
    "sarima_seasonal_order = auto_arima_model.seasonal_order\n",
    "\n",
    "sarima_model = SARIMAX(y_train, order=sarima_order, seasonal_order=sarima_seasonal_order)\n",
    "\n",
    "sarima_results = sarima_model.fit()\n",
    "\n",
    "sarima_pred = sarima_results.forecast(steps=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mis62jZ0K6cg"
   },
   "outputs": [],
   "source": [
    "# Exponential Smoothing model creation and prediction\n",
    "\n",
    "# trend='add': Use an additive trend component\n",
    "# seasonal='add': Use an additive seasonal component\n",
    "exp_smoothing = ExponentialSmoothing(y_train, seasonal_periods=7, trend='add', seasonal='add')\n",
    "\n",
    "exp_smoothing_results = exp_smoothing.fit()\n",
    "\n",
    "exp_smoothing_pred = exp_smoothing_results.forecast(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i03hEuUisiKe"
   },
   "source": [
    "## Ensemble Creation\n",
    "### Voting Regressor Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "QK2STaFlK6Xl",
    "outputId": "0b3f4ed1-b7ce-4821-ad30-b39ac84ef106"
   },
   "outputs": [],
   "source": [
    "# Creating an ensemble model using VotingRegressor\n",
    "ensemble = VotingRegressor([\n",
    "    ('gb', gb_best),\n",
    "    ('xgb', xgb_best),\n",
    "    ('lgb', lgb_best),\n",
    "    ('elastic_net', elastic_net)\n",
    "], weights=[2, 2, 2, 1])  # Assigning weights to each model\n",
    "\n",
    "ensemble.fit(X_train, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtKlgH4siKe"
   },
   "source": [
    "## 5. Model Training and Prediction\n",
    "### 5.1 Data Preparation and Splitting\n",
    "### 5.2 Model Fitting\n",
    "### 5.3 Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsI32Wp4OyHr",
    "outputId": "367d0130-aa0b-4bc7-bec0-813f68ace6c4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop('7d_ROI', axis=1)\n",
    "y = df['7d_ROI']\n",
    "\n",
    "# Performing feature selection\n",
    "selected_features = advanced_feature_selection(X, y)\n",
    "X = X[selected_features]\n",
    "\n",
    "# First split the data without sample weights\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Calculate sample weights based on training data\n",
    "sample_weights = np.ones(len(X_train))  # Using uniform weights\n",
    "# Or use this if you want balanced weights:\n",
    "# sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# Print shapes to verify alignment\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"sample_weights shape:\", sample_weights.shape)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_best.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# XGBoost\n",
    "xgb_best.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# LightGBM\n",
    "lgb_best.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Elastic Net\n",
    "elastic_net.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Making predictions using each model\n",
    "gb_pred = gb_best.predict(X_test)\n",
    "xgb_pred = xgb_best.predict(X_test)\n",
    "lgb_pred = lgb_best.predict(X_test)\n",
    "elastic_net_pred = elastic_net.predict(X_test)\n",
    "\n",
    "# LSTM prediction\n",
    "lstm_pred = lstm_model.predict(X_test_lstm).flatten()\n",
    "\n",
    "# SARIMA prediction\n",
    "sarima_pred = sarima_results.forecast(steps=len(X_test))\n",
    "\n",
    "# Exponential Smoothing prediction\n",
    "exp_smoothing_pred = exp_smoothing_results.forecast(len(X_test))\n",
    "\n",
    "# Ensemble prediction\n",
    "ensemble_pred = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbByPtSxsiKf"
   },
   "source": [
    "## 6. Model Retraining and Final Prediction\n",
    "### 6.1 Updating Dataset\n",
    "### 6.2 Model Retraining\n",
    "### 6.3 Generating Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xf82vmiAOyEx",
    "outputId": "23d42ac2-c84a-4410-f7e5-3cc8ec5f2cac"
   },
   "outputs": [],
   "source": [
    "# Updating the training and test sets\n",
    "latest_date = df.index[-1]\n",
    "test_start_date = latest_date - timedelta(days=6)\n",
    "X_train = X[X.index < test_start_date]\n",
    "X_test = X[X.index >= test_start_date]\n",
    "y_train = y[y.index < test_start_date]\n",
    "y_test = y[y.index >= test_start_date]\n",
    "\n",
    "# Retraining models on the updated training data\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_best.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "xgb_best.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM\n",
    "lgb_best.fit(X_train, y_train)\n",
    "\n",
    "# Elastic Net\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# LSTM\n",
    "X_train_lstm = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "y_train_lstm = y_train.values\n",
    "lstm_model.fit(X_train_lstm, y_train_lstm, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# SARIMA\n",
    "sarima_model = SARIMAX(y_train, order=sarima_order, seasonal_order=sarima_seasonal_order)\n",
    "sarima_results = sarima_model.fit()\n",
    "\n",
    "# Exponential Smoothing\n",
    "exp_smoothing = ExponentialSmoothing(y_train, seasonal_periods=7, trend='add', seasonal='add')\n",
    "exp_smoothing_results = exp_smoothing.fit()\n",
    "\n",
    "# Prophet\n",
    "prophet_df_train = pd.DataFrame({'ds': X_train.index, 'y': y_train})\n",
    "prophet_model = Prophet(changepoint_prior_scale=0.05, seasonality_prior_scale=10)\n",
    "prophet_model.fit(prophet_df_train)\n",
    "\n",
    "# Ensemble\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Generating predictions with the new test set\n",
    "\n",
    "# Gradient Boosting prediction\n",
    "gb_pred = gb_best.predict(X_test)\n",
    "\n",
    "# XGBoost prediction\n",
    "xgb_pred = xgb_best.predict(X_test)\n",
    "\n",
    "# LightGBM prediction\n",
    "lgb_pred = lgb_best.predict(X_test)\n",
    "\n",
    "# Elastic Net prediction\n",
    "elastic_net_pred = elastic_net.predict(X_test)\n",
    "\n",
    "# LSTM prediction\n",
    "X_test_lstm = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "lstm_pred = lstm_model.predict(X_test_lstm).flatten()\n",
    "\n",
    "# SARIMA prediction\n",
    "sarima_pred = sarima_results.forecast(steps=len(X_test))\n",
    "\n",
    "# Exponential Smoothing prediction\n",
    "exp_smoothing_pred = exp_smoothing_results.forecast(len(X_test))\n",
    "\n",
    "# Prophet prediction\n",
    "prophet_future = prophet_model.make_future_dataframe(periods=len(X_test), freq='D')\n",
    "prophet_forecast = prophet_model.predict(prophet_future)\n",
    "prophet_pred = prophet_forecast.tail(len(X_test))['yhat'].values\n",
    "\n",
    "# Ensemble prediction\n",
    "ensemble_pred = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xf82vmiAOyEx",
    "outputId": "23d42ac2-c84a-4410-f7e5-3cc8ec5f2cac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MultiTargetPredictor:\n",
    "    \"\"\"A class for training and managing multiple models on multiple targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_models=None):\n",
    "        \"\"\"\n",
    "        Initialize the multi-target predictor.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_models : dict\n",
    "            Dictionary of base models to use for each target\n",
    "        \"\"\"\n",
    "        if base_models is None:\n",
    "            self.base_models = {\n",
    "                'gradient_boosting': GradientBoostingRegressor(random_state=42),\n",
    "                'xgboost': XGBRegressor(random_state=42),\n",
    "                'elastic_net': ElasticNet(random_state=42),\n",
    "                'lightgbm': LGBMRegressor(random_state=42)\n",
    "            }\n",
    "        else:\n",
    "            self.base_models = base_models\n",
    "            \n",
    "        self.models = {}\n",
    "        self.feature_columns = None\n",
    "        self.scaler = None\n",
    "        \n",
    "    def fit(self, training_data):\n",
    "        \"\"\"\n",
    "        Fit models for each target using the provided training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        training_data : dict\n",
    "            Dictionary containing training data for each target\n",
    "        \"\"\"\n",
    "        self.feature_columns = training_data['feature_columns']\n",
    "        self.scaler = training_data['scaler']\n",
    "        \n",
    "        # Train models for each target\n",
    "        for target_name in ['expected_loss', 'log_expected_loss', 'risk_level', 'risk_adjusted_loss']:\n",
    "            if target_name not in self.models:\n",
    "                self.models[target_name] = {}\n",
    "                \n",
    "            X_train = training_data['X_train']\n",
    "            y_train = training_data['y_train']\n",
    "            \n",
    "            # Train each base model for this target\n",
    "            for model_name, base_model in self.base_models.items():\n",
    "                print(f\"\\nTraining {model_name} for {target_name}...\")\n",
    "                model = base_model.__class__(**base_model.get_params())\n",
    "                model.fit(X_train, y_train)\n",
    "                self.models[target_name][model_name] = model\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate predictions for each target using all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features to generate predictions for\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary of predictions for each target\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.feature_columns)\n",
    "            \n",
    "        # Scale features if scaler exists\n",
    "        if self.scaler is not None:\n",
    "            X = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "            \n",
    "        predictions = {}\n",
    "        for target_name, target_models in self.models.items():\n",
    "            target_preds = []\n",
    "            for model_name, model in target_models.items():\n",
    "                pred = model.predict(X)\n",
    "                target_preds.append(pred)\n",
    "            \n",
    "            # Average predictions from all models for this target\n",
    "            predictions[target_name] = np.mean(target_preds, axis=0)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "def create_weighted_prediction(individual_predictions, weights=None):\n",
    "    \"\"\"\n",
    "    Combine predictions from multiple targets using weighted average.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    individual_predictions : dict\n",
    "        Dictionary of predictions from each target\n",
    "    weights : dict, optional\n",
    "        Dictionary of weights for each target\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    array : Weighted average predictions\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        # Default weights based on prediction type\n",
    "        weights = {\n",
    "            'expected_loss': 0.4,\n",
    "            'log_expected_loss': 0.2,\n",
    "            'risk_level': 0.2,\n",
    "            'risk_adjusted_loss': 0.2\n",
    "        }\n",
    "    \n",
    "    weighted_pred = np.zeros_like(list(individual_predictions.values())[0])\n",
    "    for target_name, predictions in individual_predictions.items():\n",
    "        if target_name in weights:\n",
    "            weighted_pred += predictions * weights[target_name]\n",
    "            \n",
    "    return weighted_pred\n",
    "\n",
    "def train_and_evaluate():\n",
    "    \"\"\"\n",
    "    Train models and evaluate their performance.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (predictor, training_data)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get training data\n",
    "        training_data = get_training_data('alphaized_data.csv', target_name='expected_loss')\n",
    "        \n",
    "        # Initialize and train predictor\n",
    "        predictor = MultiTargetPredictor()\n",
    "        predictor.fit(training_data)\n",
    "        \n",
    "        # Make predictions\n",
    "        X_test = training_data['X_test']\n",
    "        predictions = predictor.predict(X_test)\n",
    "        \n",
    "        # Evaluate individual target predictions\n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        print(\"=\" * 50)\n",
    "        for target_name, target_pred in predictions.items():\n",
    "            if target_name == 'expected_loss':  # Only evaluate on main target\n",
    "                mse = mean_squared_error(training_data['y_test'], target_pred)\n",
    "                r2 = r2_score(training_data['y_test'], target_pred)\n",
    "                print(f\"\\n{target_name}:\")\n",
    "                print(f\"MSE: {mse:.6f}\")\n",
    "                print(f\"R²: {r2:.6f}\")\n",
    "        \n",
    "        # Create weighted prediction\n",
    "        final_predictions = create_weighted_prediction(predictions)\n",
    "        \n",
    "        # Evaluate weighted predictions\n",
    "        mse = mean_squared_error(training_data['y_test'], final_predictions)\n",
    "        r2 = r2_score(training_data['y_test'], final_predictions)\n",
    "        print(\"\\nWeighted Ensemble:\")\n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"R²: {r2:.6f}\")\n",
    "        \n",
    "        return predictor, training_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor, training_data = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xf82vmiAOyEx",
    "outputId": "23d42ac2-c84a-4410-f7e5-3cc8ec5f2cac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Percentage Error (MAPE).\n",
    "    Handles zero values in y_true by adding a small epsilon to avoid division by zero.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Actual values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float : MAPE value as a percentage\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Add small epsilon to avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    y_true_safe = y_true + epsilon\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100\n",
    "    \n",
    "    return mape\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Actual values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all calculated metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'mse': mean_squared_error(y_true, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'mape': calculate_mape(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_predictions(y_true, predictions, target_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate predictions for multiple targets and print results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : dict or array-like\n",
    "        True values for each target (if dict) or single target (if array)\n",
    "    predictions : dict or array-like\n",
    "        Predicted values for each target (if dict) or single target (if array)\n",
    "    target_names : list, optional\n",
    "        List of target names for printing results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics for all targets\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    all_metrics = {}\n",
    "    \n",
    "    if isinstance(predictions, dict):\n",
    "        # Multiple targets\n",
    "        for target_name, target_pred in predictions.items():\n",
    "            if target_name in y_true:\n",
    "                metrics = calculate_metrics(y_true[target_name], target_pred)\n",
    "                all_metrics[target_name] = metrics\n",
    "                \n",
    "                print(f\"\\n{target_name}:\")\n",
    "                print(f\"MSE:  {metrics['mse']:.6f}\")\n",
    "                print(f\"RMSE: {metrics['rmse']:.6f}\")\n",
    "                print(f\"MAE:  {metrics['mae']:.6f}\")\n",
    "                print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
    "                print(f\"R²:   {metrics['r2']:.6f}\")\n",
    "    else:\n",
    "        # Single target\n",
    "        metrics = calculate_metrics(y_true, predictions)\n",
    "        all_metrics['single_target'] = metrics\n",
    "        \n",
    "        target_name = target_names[0] if target_names else \"Target\"\n",
    "        print(f\"\\n{target_name}:\")\n",
    "        print(f\"MSE:  {metrics['mse']:.6f}\")\n",
    "        print(f\"RMSE: {metrics['rmse']:.6f}\")\n",
    "        print(f\"MAE:  {metrics['mae']:.6f}\")\n",
    "        print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
    "        print(f\"R²:   {metrics['r2']:.6f}\")\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "# Example usage in the training pipeline:\n",
    "def train_and_evaluate():\n",
    "    \"\"\"\n",
    "    Train models and evaluate their performance with comprehensive metrics.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (predictor, training_data, metrics)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get training data\n",
    "        training_data = get_training_data('alphaized_data.csv', target_name='expected_loss')\n",
    "        \n",
    "        # Initialize and train predictor\n",
    "        predictor = MultiTargetPredictor()\n",
    "        predictor.fit(training_data)\n",
    "        \n",
    "        # Make predictions\n",
    "        X_test = training_data['X_test']\n",
    "        predictions = predictor.predict(X_test)\n",
    "        \n",
    "        # Evaluate predictions\n",
    "        metrics = evaluate_predictions(\n",
    "            {'expected_loss': training_data['y_test']},\n",
    "            {k: v for k, v in predictions.items() if k == 'expected_loss'}\n",
    "        )\n",
    "        \n",
    "        # Create and evaluate weighted predictions\n",
    "        final_predictions = create_weighted_prediction(predictions)\n",
    "        weighted_metrics = calculate_metrics(training_data['y_test'], final_predictions)\n",
    "        \n",
    "        print(\"\\nWeighted Ensemble:\")\n",
    "        print(f\"MSE:  {weighted_metrics['mse']:.6f}\")\n",
    "        print(f\"RMSE: {weighted_metrics['rmse']:.6f}\")\n",
    "        print(f\"MAE:  {weighted_metrics['mae']:.6f}\")\n",
    "        print(f\"MAPE: {weighted_metrics['mape']:.2f}%\")\n",
    "        print(f\"R²:   {weighted_metrics['r2']:.6f}\")\n",
    "        \n",
    "        metrics['weighted_ensemble'] = weighted_metrics\n",
    "        \n",
    "        return predictor, training_data, metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example of using the metrics separately:\n",
    "def evaluate_model_performance(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a single model's performance with all metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    model_name : str, optional\n",
    "        Name of the model for printing results\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all calculated metrics\n",
    "    \"\"\"\n",
    "    metrics = calculate_metrics(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MSE:  {metrics['mse']:.6f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.6f}\")\n",
    "    print(f\"MAE:  {metrics['mae']:.6f}\")\n",
    "    print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
    "    print(f\"R²:   {metrics['r2']:.6f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train and evaluate with all metrics\n",
    "    predictor, training_data, all_metrics = train_and_evaluate()\n",
    "    \n",
    "    # Example of evaluating a single prediction\n",
    "    y_true = training_data['y_test']\n",
    "    y_pred = predictor.predict(training_data['X_test'])['expected_loss']\n",
    "    single_metrics = evaluate_model_performance(y_true, y_pred, \"Single Target Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKEtbnWksiKf"
   },
   "source": [
    "## 7. Dynamic Ensemble Prediction\n",
    "### 7.1 Weight Calculation Function\n",
    "### 7.2 Prediction Aggregation\n",
    "### 7.3 Final Prediction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JolsffBDOyCP",
    "outputId": "5c992fcb-f121-428a-e528-f76a61d9566f"
   },
   "outputs": [],
   "source": [
    "def dynamic_ensemble_weights(predictions, y_test):\n",
    "    \"\"\"\n",
    "    Calculating dynamic weights for ensemble predictions based on absolute errors.\n",
    "    \"\"\"\n",
    "    # Calculating absolute errors for each model's predictions\n",
    "    errors = np.abs(np.array(predictions) - y_test.values.reshape(-1, 1))\n",
    "\n",
    "    # Calculating weights as inverse of errors (with small constant to avoid division by zero)\n",
    "    weights = 1 / (errors + 1e-8)\n",
    "\n",
    "    # Normalizing weights so they sum to 1 for each sample\n",
    "    weights = weights / np.sum(weights, axis=1, keepdims=True)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Stack predictions from different models into a single array\n",
    "predictions = np.column_stack([\n",
    "    gb_pred, xgb_pred, lgb_pred, elastic_net_pred,\n",
    "    lstm_pred, sarima_pred, exp_smoothing_pred,\n",
    "    ensemble_pred\n",
    "])\n",
    "\n",
    "# Calculating dynamic weights based on prediction errors\n",
    "weights = dynamic_ensemble_weights(predictions, y_test)\n",
    "\n",
    "# Computing final predictions as weighted sum of individual model predictions\n",
    "y_pred = np.sum(predictions * weights, axis=1)\n",
    "\n",
    "print(f\"Evaluating model on data from {test_start_date} to {latest_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtqkKtMnsiKl"
   },
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PkJrBrGOx_u",
    "outputId": "63e244f7-b6b9-473c-ba72-6a2f0d2f806f"
   },
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "# Printing evaluation metrics\n",
    "print(f'Mean Absolute Error: {mae:.6f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.6f}')\n",
    "print(f'R-squared: {r2:.6f}')\n",
    "print(f'Mean Absolute Percentage Error: {mape:.6f}')\n",
    "\n",
    "def fit_model_with_error_handling(model, X, y):\n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_with_error_handling(model, X):\n",
    "    if model is None:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return model.predict(X)\n",
    "    except Exception as e:\n",
    "        print(f\"Error making prediction: {e}\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyoI4weTOx9F"
   },
   "outputs": [],
   "source": [
    "gb_best = fit_model_with_error_handling(gb_best, X_train, y_train)\n",
    "gb_pred = predict_with_error_handling(gb_best, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhpfOfaosiKl"
   },
   "source": [
    "### 9. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Initial Data Analysis:\n",
      "   - Spread: 5.0%\n",
      "   - Expected Loss: 2.0%\n",
      "   - Risk Score: 75.0%\n",
      "   - First Loss Probability: 15.0%\n",
      "   - Temperature Anomaly: +10.5°C\n",
      "\n",
      "\n",
      "2. Environmental Risk Calculation:\n",
      "   - Base Hurricane Temperature: 26.5°C\n",
      "   - Current Temperature Factor: 190.066\n",
      "\n",
      "\n",
      "3. Adjusted Loss Probability:\n",
      "   - Base First Loss Probability: 15.0%\n",
      "   - Adjusted Loss Probability: 100.00%\n",
      "\n",
      "\n",
      "4. Financial Metrics:\n",
      "   - Original Spread: 5.0%\n",
      "   - Risk-Adjusted Spread: 717.75%\n",
      "\n",
      "\n",
      "5. Final Assessment:\n",
      "   - Risk Level (1-5): 5\n",
      "   - Relative Value: 1.50x\n",
      "   - Recommendation: Sell\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CatBondAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Initialize base parameters\n",
    "        self.base_hurricane_temp = 26.5  # Base temperature for hurricane formation (°C)\n",
    "        self.risk_multiplier = 1.15      # Risk multiplier for above-average temps\n",
    "        \n",
    "    def analyze_hurricane_bond(self, \n",
    "                             spread_pct=5.0,           # Spread in percentage\n",
    "                             expected_loss_pct=2.0,    # Expected loss in percentage\n",
    "                             risk_score=0.75,          # Medium-high risk (0-1 scale)\n",
    "                             first_loss_prob=0.15,     # First loss probability\n",
    "                             temp_anomaly=1.5):        # Temperature above average (°C)\n",
    "        \n",
    "        print(\"1. Initial Data Analysis:\")\n",
    "        print(f\"   - Spread: {spread_pct}%\")\n",
    "        print(f\"   - Expected Loss: {expected_loss_pct}%\")\n",
    "        print(f\"   - Risk Score: {risk_score*100}%\")\n",
    "        print(f\"   - First Loss Probability: {first_loss_prob*100}%\")\n",
    "        print(f\"   - Temperature Anomaly: +{temp_anomaly}°C\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # 1. Environmental Risk Adjustment\n",
    "        temp_risk_factor = self._calculate_temp_risk(temp_anomaly)\n",
    "        print(\"2. Environmental Risk Calculation:\")\n",
    "        print(f\"   - Base Hurricane Temperature: {self.base_hurricane_temp}°C\")\n",
    "        print(f\"   - Current Temperature Factor: {temp_risk_factor:.3f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # 2. Insurance Risk Calculations\n",
    "        adjusted_loss_prob = self._calculate_adjusted_loss_prob(\n",
    "            first_loss_prob, \n",
    "            temp_risk_factor, \n",
    "            risk_score\n",
    "        )\n",
    "        print(\"3. Adjusted Loss Probability:\")\n",
    "        print(f\"   - Base First Loss Probability: {first_loss_prob*100}%\")\n",
    "        print(f\"   - Adjusted Loss Probability: {adjusted_loss_prob*100:.2f}%\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # 3. Financial Metrics\n",
    "        risk_adjusted_spread = self._calculate_risk_adjusted_spread(\n",
    "            spread_pct,\n",
    "            expected_loss_pct,\n",
    "            temp_risk_factor,\n",
    "            risk_score\n",
    "        )\n",
    "        print(\"4. Financial Metrics:\")\n",
    "        print(f\"   - Original Spread: {spread_pct}%\")\n",
    "        print(f\"   - Risk-Adjusted Spread: {risk_adjusted_spread:.2f}%\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # 4. Final Risk Assessment\n",
    "        final_metrics = self._calculate_final_metrics(\n",
    "            spread_pct,\n",
    "            expected_loss_pct,\n",
    "            adjusted_loss_prob,\n",
    "            risk_adjusted_spread\n",
    "        )\n",
    "        \n",
    "        return final_metrics\n",
    "\n",
    "    def _calculate_temp_risk(self, temp_anomaly):\n",
    "        \"\"\"Calculate risk factor based on temperature anomaly\"\"\"\n",
    "        # Exponential risk increase with temperature\n",
    "        return np.exp(temp_anomaly / 2) - 0.5\n",
    "        \n",
    "    def _calculate_adjusted_loss_prob(self, base_prob, temp_factor, risk_score):\n",
    "        \"\"\"Calculate adjusted loss probability\"\"\"\n",
    "        return min(1.0, base_prob * temp_factor * (1 + risk_score))\n",
    "        \n",
    "    def _calculate_risk_adjusted_spread(self, spread, exp_loss, temp_factor, risk_score):\n",
    "        \"\"\"Calculate risk-adjusted spread\"\"\"\n",
    "        return spread * (1 + (temp_factor * risk_score))\n",
    "\n",
    "    def _calculate_final_metrics(self, spread, exp_loss, adj_prob, risk_adj_spread):\n",
    "        \"\"\"Calculate final risk metrics\"\"\"\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        relative_value = (spread - exp_loss) / exp_loss\n",
    "        risk_score = (adj_prob * risk_adj_spread) / spread\n",
    "        \n",
    "        # Risk level classification (1-5)\n",
    "        risk_level = min(5, max(1, int(risk_score * 5)))\n",
    "        \n",
    "        # Investment recommendation\n",
    "        if relative_value > 2.5 and risk_level <= 3:\n",
    "            recommendation = \"Strong Buy\"\n",
    "        elif relative_value > 1.5 and risk_level <= 4:\n",
    "            recommendation = \"Buy\"\n",
    "        elif relative_value < 0.5 or risk_level == 5:\n",
    "            recommendation = \"Sell\"\n",
    "        else:\n",
    "            recommendation = \"Hold\"\n",
    "\n",
    "        metrics = {\n",
    "            \"risk_level\": risk_level,\n",
    "            \"relative_value\": relative_value,\n",
    "            \"adjusted_probability\": adj_prob,\n",
    "            \"risk_adjusted_spread\": risk_adj_spread,\n",
    "            \"recommendation\": recommendation\n",
    "        }\n",
    "        \n",
    "        print(\"5. Final Assessment:\")\n",
    "        print(f\"   - Risk Level (1-5): {risk_level}\")\n",
    "        print(f\"   - Relative Value: {relative_value:.2f}x\")\n",
    "        print(f\"   - Recommendation: {recommendation}\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Run analysis for the example hurricane bond\n",
    "analyzer = CatBondAnalyzer()\n",
    "results = analyzer.analyze_hurricane_bond(\n",
    "    spread_pct=5.0,           # 5% spread\n",
    "    expected_loss_pct=2.0,    # 2% expected loss\n",
    "    risk_score=0.75,          # Medium-high risk (75%)\n",
    "    first_loss_prob=0.15,     # 15% first loss probability\n",
    "    temp_anomaly=1.5          # 1.5°C above average\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21768]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:5049 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Initial Data Analysis:\n",
      "   - Spread: 11.0%\n",
      "   - Expected Loss: 11.0%\n",
      "   - Risk Score: 1100.0%\n",
      "   - First Loss Probability: 1100.0%\n",
      "   - Temperature Anomaly: +11.0°C\n",
      "\n",
      "\n",
      "2. Environmental Risk Calculation:\n",
      "   - Base Hurricane Temperature: 26.5°C\n",
      "   - Current Temperature Factor: 244.192\n",
      "\n",
      "\n",
      "3. Adjusted Loss Probability:\n",
      "   - Base First Loss Probability: 1100.0%\n",
      "   - Adjusted Loss Probability: 100.00%\n",
      "\n",
      "\n",
      "4. Financial Metrics:\n",
      "   - Original Spread: 11.0%\n",
      "   - Risk-Adjusted Spread: 29558.22%\n",
      "\n",
      "\n",
      "5. Final Assessment:\n",
      "   - Risk Level (1-5): 5\n",
      "   - Relative Value: 0.00x\n",
      "   - Recommendation: Sell\n",
      "INFO:     127.0.0.1:62365 - \"POST /analyze HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52457 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n",
      "1. Initial Data Analysis:\n",
      "   - Spread: 1.0%\n",
      "   - Expected Loss: 1.0%\n",
      "   - Risk Score: 100.0%\n",
      "   - First Loss Probability: 100.0%\n",
      "   - Temperature Anomaly: +1.0°C\n",
      "\n",
      "\n",
      "2. Environmental Risk Calculation:\n",
      "   - Base Hurricane Temperature: 26.5°C\n",
      "   - Current Temperature Factor: 1.149\n",
      "\n",
      "\n",
      "3. Adjusted Loss Probability:\n",
      "   - Base First Loss Probability: 100.0%\n",
      "   - Adjusted Loss Probability: 100.00%\n",
      "\n",
      "\n",
      "4. Financial Metrics:\n",
      "   - Original Spread: 1.0%\n",
      "   - Risk-Adjusted Spread: 2.15%\n",
      "\n",
      "\n",
      "5. Final Assessment:\n",
      "   - Risk Level (1-5): 5\n",
      "   - Relative Value: 0.00x\n",
      "   - Recommendation: Sell\n",
      "INFO:     127.0.0.1:52457 - \"POST /analyze HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# Required for running FastAPI in Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI()\n",
    "origins = [\"*\"]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "analyzer = CatBondAnalyzer()\n",
    "\n",
    "# Pydantic model for input data\n",
    "class BondAnalysisRequest(BaseModel):\n",
    "    spread_pct: float\n",
    "    expected_loss_pct: float\n",
    "    risk_score: float\n",
    "    first_loss_prob: float\n",
    "    temp_anomaly: float\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\n",
    "        \"message\": \"Welcome to the Cat Bond Analyzer API!\",\n",
    "        \"endpoints\": {\n",
    "            \"/analyze\": \"POST - Analyze a hurricane bond with input parameters.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Endpoint to analyze hurricane bond\n",
    "@app.post(\"/analyze\")\n",
    "def analyze_bond(data: BondAnalysisRequest):\n",
    "    \"\"\"Analyze a hurricane bond based on provided parameters.\"\"\"\n",
    "    results = analyzer.analyze_hurricane_bond(\n",
    "        spread_pct=data.spread_pct,\n",
    "        expected_loss_pct=data.expected_loss_pct,\n",
    "        risk_score=data.risk_score,\n",
    "        first_loss_prob=data.first_loss_prob,\n",
    "        temp_anomaly=data.temp_anomaly\n",
    "    )\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=5049)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for better visualization\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def create_model_comparison_plots(y_true, predictions_dict, model_names):\n",
    "    \"\"\"\n",
    "    Create comparison plots for different models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    predictions_dict : dict\n",
    "        Dictionary containing predictions from different models\n",
    "    model_names : list\n",
    "        List of model names\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Model Performance Comparison Bar Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics = {\n",
    "        'MAE': [mean_absolute_error(y_true, pred) for pred in predictions_dict.values()],\n",
    "        'RMSE': [np.sqrt(mean_squared_error(y_true, pred)) for pred in predictions_dict.values()],\n",
    "        'R2': [r2_score(y_true, pred) for pred in predictions_dict.values()]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    ax.bar(x - width, metrics['MAE'], width, label='MAE')\n",
    "    ax.bar(x, metrics['RMSE'], width, label='RMSE')\n",
    "    ax.bar(x + width, metrics['R2'], width, label='R²')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Prediction vs Actual Scatter Plots\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for idx, (model_name, predictions) in enumerate(predictions_dict.items(), 1):\n",
    "        plt.subplot(2, (len(model_names)+1)//2, idx)\n",
    "        plt.scatter(y_true, predictions, alpha=0.5)\n",
    "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title(f'{model_name} Predictions vs Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Residual Plots\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for idx, (model_name, predictions) in enumerate(predictions_dict.items(), 1):\n",
    "        residuals = y_true - predictions\n",
    "        plt.subplot(2, (len(model_names)+1)//2, idx)\n",
    "        plt.scatter(predictions, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predicted Values')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title(f'{model_name} Residual Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Error Distribution\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for idx, (model_name, predictions) in enumerate(predictions_dict.items(), 1):\n",
    "        residuals = y_true - predictions\n",
    "        plt.subplot(2, (len(model_names)+1)//2, idx)\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.xlabel('Prediction Error')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'{model_name} Error Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_time_series_plots(dates, y_true, predictions_dict, model_names):\n",
    "    \"\"\"\n",
    "    Create time series plots for model predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dates : array-like\n",
    "        Dates for the time series\n",
    "    y_true : array-like\n",
    "        True values\n",
    "    predictions_dict : dict\n",
    "        Dictionary containing predictions from different models\n",
    "    model_names : list\n",
    "        List of model names\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Time Series Plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(dates, y_true, label='Actual', linewidth=2)\n",
    "    for model_name, predictions in predictions_dict.items():\n",
    "        plt.plot(dates, predictions, '--', label=f'{model_name} Predictions', alpha=0.7)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Time Series Predictions Comparison')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Rolling Mean Error\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    window = 30  # 30-day rolling window\n",
    "    for model_name, predictions in predictions_dict.items():\n",
    "        error = np.abs(y_true - predictions)\n",
    "        rolling_mean_error = pd.Series(error).rolling(window=window).mean()\n",
    "        plt.plot(dates, rolling_mean_error, label=f'{model_name}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Rolling Mean Absolute Error')\n",
    "    plt.title(f'{window}-Day Rolling Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_feature_importance_plot(model, feature_names):\n",
    "    \"\"\"\n",
    "    Create feature importance plot for tree-based models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : object\n",
    "        Trained model object (must have feature_importances_ attribute)\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.title('Feature Importances')\n",
    "        plt.bar(range(len(indices)), importances[indices])\n",
    "        plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_correlation_matrix(X, feature_names):\n",
    "    \"\"\"\n",
    "    Create correlation matrix heatmap\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    \"\"\"\n",
    "    \n",
    "    corr_matrix = pd.DataFrame(X, columns=feature_names).corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dictionary of predictions\n",
    "    predictions_dict = {\n",
    "        'Gradient Boosting': gb_pred,\n",
    "        'XGBoost': xgb_pred,\n",
    "        'LightGBM': lgb_pred,\n",
    "        'Elastic Net': elastic_net_pred,\n",
    "        'LSTM': lstm_pred,\n",
    "        'SARIMA': sarima_pred,\n",
    "        'Exponential Smoothing': exp_smoothing_pred,\n",
    "        'Ensemble': ensemble_pred\n",
    "    }\n",
    "    \n",
    "    # Create model comparison plots\n",
    "    create_model_comparison_plots(y_test, predictions_dict, list(predictions_dict.keys()))\n",
    "    \n",
    "    # Create time series plots\n",
    "    create_time_series_plots(X_test.index, y_test, predictions_dict, list(predictions_dict.keys()))\n",
    "    \n",
    "    # Create feature importance plot for GB model\n",
    "    create_feature_importance_plot(gb_best, X_train.columns)\n",
    "    \n",
    "    # Create correlation matrix plot\n",
    "    plot_correlation_matrix(X_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
